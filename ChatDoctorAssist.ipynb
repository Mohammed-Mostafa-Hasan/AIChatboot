{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a4ab4cc",
   "metadata": {},
   "source": [
    "## Build a conversation agent by langchain and OpenAI \n",
    "- overview about langchain \n",
    "\n",
    "* it contain backbone where all abstraction and runtime logic are stored\n",
    "* layer of third party integration and components \n",
    "* a set of prebuilt architectures and templates to use\n",
    "* a serving layer to consume chain as api\n",
    "* an observability layer to monitor your application in development test and and production state \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19484362",
   "metadata": {},
   "source": [
    "### Overview about all libraries used \n",
    "\n",
    "- os : interacting with our operating system to get data from it.\n",
    "- dotenv: .env file is a file where i can store my secret info in .gitignore file\n",
    "\n",
    "<a href = \"https://docs.langchain.com/?_gl=1*1qfiw4g*_gcl_au*MTkyNDcwMTM3Ny4xNzYyMzE4Mzgz*_ga*NzM3NDc0ODQyLjE3NjIzMTgzODQ.*_ga_47WX3HKKY2*czE3NjIzMTgzODMkbzEkZzAkdDE3NjIzMTgzODMkajYwJGwwJGgw\">Langchain</a>\n",
    "\n",
    "\n",
    "\n",
    "- <font color =\"yellow\">Prompt Templates </font>: help to translate user input and parameters into instructions for a language model. This can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output for more information click on <a href = \"https://github.com/langchain-ai/langchain/blob/v0.3/docs/docs/concepts/prompt_templates.mdx\">link</a> this can imported from langchain or langchain-classic.\n",
    "\n",
    "- <font color =\"yellow\">LLmChain </font>: Itâ€™s used to connect a language model (like GPT) with a prompt template â€” making it easy to run structured interactions with an LLM this can imported from langchain or langchain-classic.\n",
    "\n",
    "Benefit of usage\n",
    "\n",
    "âœ… Reusability â€” easily reuse prompt templates.\n",
    "\n",
    "âœ… Integration â€” can chain multiple LLMChains together.\n",
    "\n",
    "âœ… Clarity â€” separates prompt design from model logic.\n",
    "\n",
    "âœ… Control â€” better structure for debugging and scaling workflows.\n",
    "\n",
    "- <font color =\"yellow\">ConversationChain </font>: maintained conversation history with memory. It combined a language model with a memory component to track the conversation context can imported from langchain.chains .\n",
    "\n",
    "- <font color =\"yellow\">ChatOpenAI </font> This guide will help you getting started with ChatOpenAI chat models for more <a href = \"https://v03.api.js.langchain.com/classes/_langchain_openai.ChatOpenAI.html?_gl=1*z6uc5g*_gcl_au*MTkyNDcwMTM3Ny4xNzYyMzE4Mzgz*_ga*NzM3NDc0ODQyLjE3NjIzMTgzODQ.*_ga_47WX3HKKY2*czE3NjIzMjU3OTQkbzMkZzEkdDE3NjIzMjU4MTAkajQ0JGwwJGgw\">info</a> this can import from langchain.chat_models .\n",
    "\n",
    "- <font color =\"yellow\">ConversationBufferMemory </font> this enable you to store your chat message and extract them in a variable to use it when it needed can imported from langchain.memory . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee190dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain-core in /Users/mac/Library/Python/3.9/lib/python/site-packages (0.3.79)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /Users/mac/Library/Python/3.9/lib/python/site-packages (from langchain-core) (25.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/mac/Library/Python/3.9/lib/python/site-packages (from langchain-core) (2.12.3)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /Users/mac/Library/Python/3.9/lib/python/site-packages (from langchain-core) (4.15.0)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /Users/mac/Library/Python/3.9/lib/python/site-packages (from langchain-core) (6.0.3)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /Users/mac/Library/Python/3.9/lib/python/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /Users/mac/Library/Python/3.9/lib/python/site-packages (from langchain-core) (0.4.37)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/mac/Library/Python/3.9/lib/python/site-packages (from langchain-core) (9.1.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/mac/Library/Python/3.9/lib/python/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/mac/Library/Python/3.9/lib/python/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /Users/mac/Library/Python/3.9/lib/python/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /Users/mac/Library/Python/3.9/lib/python/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/mac/Library/Python/3.9/lib/python/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /Users/mac/Library/Python/3.9/lib/python/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.4)\n",
      "Requirement already satisfied: anyio in /Users/mac/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (4.11.0)\n",
      "Requirement already satisfied: idna in /Users/mac/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (3.11)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/mac/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.9)\n",
      "Requirement already satisfied: certifi in /Users/mac/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2025.10.5)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/mac/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (0.16.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /Users/mac/Library/Python/3.9/lib/python/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/mac/Library/Python/3.9/lib/python/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.4.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/mac/Library/Python/3.9/lib/python/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.7.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mac/Library/Python/3.9/lib/python/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/mac/Library/Python/3.9/lib/python/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (3.4.4)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/mac/Library/Python/3.9/lib/python/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/mac/Library/Python/3.9/lib/python/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (1.3.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "''' all libraries installed \n",
    "pip install langchain-core\n",
    "pip install faiss-cpu google-search-results\n",
    "pip install tiktoken\n",
    "pip install pypdf\n",
    "pip install openai\n",
    "pip install huggingface_hub\n",
    "pip install streamlit\n",
    " pip install --upgrade langchain langchain-community langchain-openai  # Install core, community, and OpenAI integrations\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3787a605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import OpenAI \n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "OpenAI_api_key = os.environ['openai_api_key']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9107ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=OpenAI_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "224c82ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure thing!â€¯\n",
      "\n",
      "Why donâ€™t scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  extra_headers={\n",
    "    \"HTTP-Referer\": \"https://www.youtube.com\", # Optional. Site URL for rankings on openrouter.ai.\n",
    "    \"X-Title\": \"getconnection tools\", # Optional. Site title for rankings on openrouter.ai.\n",
    "  },\n",
    "  model=\"openai/gpt-oss-20b\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"tell me a jok?\"\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd0cef57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "from langchain.chains import LLMChain, ConversationChain  \n",
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "314ed17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hereâ€™s a **â€œRome in 48â€¯hoursâ€** itinerary that balances the mustâ€‘see landmarks with a few hidden gems, all while keeping travel time to a minimum. Feel free to swap a few spots if you have a particular interest (art, food, nightlife, etc.).\n",
      "\n",
      "---\n",
      "\n",
      "## Dayâ€¯1 â€“ Ancient Rome & Classic Highlights\n",
      "\n",
      "| Time | Activity | Notes |\n",
      "|------|----------|-------|\n",
      "| **08:30 â€“ 10:30** | **Colosseum** (skipâ€‘theâ€‘line ticket) | Arrive early to beat the crowds. 30â€‘min guided audio tour is highly recommended. |\n",
      "| **10:45 â€“ 12:15** | **Roman Forum & Palatine Hill** | Walk through the heart of the Republic. 30â€‘min audio guide. |\n",
      "| **12:30 â€“ 13:30** | **Lunch near Monti** | Try a classic *pizza al taglio* or *supplÃ¬* at a local trattoria (e.g., *Pizzeria Monti*). |\n",
      "| **13:45 â€“ 14:30** | **Capitoline Hill & Museums** | Short walk; the museums are a bit quieter in the afternoon. |\n",
      "| **14:45 â€“ 15:30** | **Pantheon** | Free entry. 15â€‘min walk from the Capitoline. |\n",
      "| **15:45 â€“ 16:15** | **Piazza Navona** | Grab a gelato, enjoy the fountains, and maybe a quick coffee at *CaffÃ¨ Greco*. |\n",
      "| **16:30 â€“ 17:00** | **Trevi Fountain** | Best in the late afternoon light; toss a coin! |\n",
      "| **17:15 â€“ 18:00** | **Spanish Steps** | Stroll up, enjoy the view, and maybe a quick espresso. |\n",
      "| **18:15 â€“ 19:30** | **Dinner in Trastevere** | Walk across the Tiber; *Da Enzo al 29* or *Osteria der Belli* are great choices. |\n",
      "| **20:00 â€“ 21:30** | **Optional: Night walk** | The city lights up beautifully; consider a short walk to the *Piazza San Pietro* area for a nighttime view of St. Peterâ€™s. |\n",
      "\n",
      "**Transport tips:**  \n",
      "- The Colosseum, Forum, and Palatine are all within a 10â€‘minute walk of each other.  \n",
      "- From the Forum to Trastevere, take the metro Line A (Colosseo â†’ Trastevere) â€“ 10â€‘min ride.  \n",
      "- Walking is the best way to soak in Romeâ€™s atmosphere; keep comfortable shoes.\n",
      "\n",
      "---\n",
      "\n",
      "## Dayâ€¯2 â€“ Vatican & Baroque Rome\n",
      "\n",
      "| Time | Activity | Notes |\n",
      "|------|----------|-------|\n",
      "| **08:00 â€“ 10:30** | **Vatican Museums & Sistine Chapel** (skipâ€‘theâ€‘line ticket) | Arrive at opening time (9:00). 2â€‘hour tour is typical. |\n",
      "| **10:45 â€“ 11:30** | **St.â€¯Peterâ€™s Basilica** | Free entry; climb to the dome for a panoramic view (extra fee). |\n",
      "| **11:45 â€“ 12:30** | **Castel Santâ€™Angelo** | Walk along the Tiber; the museum offers great views of the city. |\n",
      "| **12:45 â€“ 13:45** | **Lunch near Piazza Navona** | Try *La Tavernaccia* or a *panino* from a street vendor. |\n",
      "| **14:00 â€“ 15:00** | **Basilica di Santa Maria Maggiore** | One of Romeâ€™s major basilicas; free entry. |\n",
      "| **15:15 â€“ 16:00** | **Basilica di San Giovanni in Laterano** | The cathedral of Rome; free entry. |\n",
      "| **16:15 â€“ 17:00** | **Basilica di Sanâ€¯Paolo fuori le Mura** | A bit further out; worth a quick visit if time allows. |\n",
      "| **17:15 â€“ 18:00** | **Piazza del Popolo** | Walk up the hill; enjoy the twin churches and the view. |\n",
      "| **18:15 â€“ 19:30** | **Dinner in Campo deâ€™ Fiori** | *Ristorante La Pergola* (if youâ€™re up for a splurge) or a casual *pizzeria* in the square. |\n",
      "| **20:00 â€“ 21:30** | **Evening stroll** | Finish with a relaxed walk along the Tiber or a quick stop at the *Piazza di Spagna* for a nightâ€‘time photo. |\n",
      "\n",
      "**Transport tips:**  \n",
      "- The Vatican area is best reached by metro Line A (Ottaviano).  \n",
      "- From the Vatican to Campo deâ€™ Fiori, take the metro Line B (Ottaviano â†’ Cavour) â€“ 10â€‘min ride.  \n",
      "- Walking between the basilicas is manageable; use a map or a navigation app to keep the route efficient.\n",
      "\n",
      "---\n",
      "\n",
      "## Quickâ€‘Reference Checklist\n",
      "\n",
      "| Category | Mustâ€‘See | Optional / Hidden Gems |\n",
      "|----------|----------|------------------------|\n",
      "| **Ancient** | Colosseum, Forum, Palatine, Capitoline | Basilica di San Clemente (underground layers) |\n",
      "| **Vatican** | Museums, Sistine Chapel, St.â€¯Peterâ€™s | Castel Santâ€™Angelo, Basilica di Sanâ€¯Paolo fuori le Mura |\n",
      "| **Baroque** | Pantheon, Piazza Navona, Trevi, Spanish Steps | Piazza del Popolo, Campo deâ€™ Fiori |\n",
      "| **Food** | Trastevere, Trattoria in Monti, Gelato at Giolitti | Osteria der Belli, La Pergola (if budget allows) |\n",
      "| **Nightlife** | Evening walk to St.â€¯Peterâ€™s, Trastevere bars | Rooftop bar at *Terrazza Borromini* (optional) |\n",
      "\n",
      "---\n",
      "\n",
      "### Tips for a Smooth Trip\n",
      "\n",
      "1. **Tickets** â€“ Buy skipâ€‘theâ€‘line tickets online for the Colosseum and Vatican Museums. The *Roma Pass* (48â€‘hour) gives free entry to many sites and free metro rides; compare prices to see if itâ€™s worth it for you.\n",
      "2. **Dress Code** â€“ For basilicas, modest clothing (no shorts, sleeveless tops) is required. Bring a light jacket for the cooler evenings.\n",
      "3. **Hydration** â€“ Carry a refillable water bottle; Rome has many public fountains (the *Acqua Vergine*).\n",
      "4. **Safety** â€“ Keep an eye on your belongings, especially in crowded tourist spots.\n",
      "5. **Flexibility** â€“ If youâ€™re a history buff, you might want to spend more time in the Forum or the Vatican. If youâ€™re a foodie, consider swapping a museum visit for a cooking class or a food tour.\n",
      "\n",
      "Enjoy your whirlwind adventure in the Eternal City! If youâ€™d like to tweak the plan (e.g., add a day trip to Tivoli or a focus on art museums), just let me know. Buon viaggio!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize ChatOpenAI with OpenRouter parameters\n",
    "chat = ChatOpenAI(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    temperature=0,\n",
    "    api_key=OpenAI_api_key,\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    # You can also add extra headers if needed, e.g.:\n",
    "    # extra_headers={\"HTTP-Referer\": \"https://www.youtube.com\", \"X-Title\": \"getconnection tools\"},\n",
    ")\n",
    "\n",
    "# Example Usage\n",
    "messages = [\n",
    "    SystemMessage(content=\"you are a helpful assistant that help the user to plan an optimized itinerary.\")\n",
    "    ,\n",
    "    HumanMessage(\n",
    "        content=\"i am going to Rome for 2 days, what can I visit\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Invoke the model\n",
    "response = chat.invoke(messages)\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c063d1",
   "metadata": {},
   "source": [
    "### Add Memory\n",
    "keep track previous conversation with the user for better flow up response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da604d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hey there! ðŸ‘‹ Iâ€™m your friendly neighborhood AI, ready to chat, answer questions, or just keep you company. Iâ€™m powered by the latest GPTâ€‘4 architecture, and Iâ€™ve been trained on a massive mix of books, articles, websites, and even some fun triviaâ€”so feel free to throw anything my way. Whether youâ€™re curious about the science behind photosynthesis, need a recipe tweak, or just want to hear a joke, Iâ€™ve got you covered. \\n\\nBy the way, itâ€™s currently November 28, 2025, so if youâ€™re planning a holiday or just wondering whatâ€™s trending this week, I can help with that too. Whatâ€™s on your mind today?'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain  \n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=chat, verbose=True, memory=memory,\n",
    ")\n",
    "\n",
    "conversation.run(\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f468cd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on ConversationChain in module langchain.chains.conversation.base object:\n",
      "\n",
      "class ConversationChain(langchain.chains.llm.LLMChain)\n",
      " |  ConversationChain(*args: Any, name: Optional[str] = None, memory: langchain_core.memory.BaseMemory = <factory>, callbacks: Union[list[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, verbose: bool = <factory>, tags: Optional[list[str]] = None, metadata: Optional[dict[str, Any]] = None, callback_manager: Optional[langchain_core.callbacks.base.BaseCallbackManager] = None, prompt: langchain_core.prompts.base.BasePromptTemplate = PromptTemplate(input_variables=['history', 'input'], input_types={}, partial_variables={}, template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:'), llm: Union[langchain_core.runnables.base.Runnable[Union[langchain_core.prompt_values.PromptValue, str, collections.abc.Sequence[Union[langchain_core.messages.base.BaseMessage, list[str], tuple[str, str], str, dict[str, Any]]]], str], langchain_core.runnables.base.Runnable[Union[langchain_core.prompt_values.PromptValue, str, collections.abc.Sequence[Union[langchain_core.messages.base.BaseMessage, list[str], tuple[str, str], str, dict[str, Any]]]], langchain_core.messages.base.BaseMessage]], output_key: str = 'response', output_parser: langchain_core.output_parsers.base.BaseLLMOutputParser = <factory>, return_final_only: bool = True, llm_kwargs: dict = <factory>, input_key: str = 'input') -> None\n",
      " |  \n",
      " |  .. deprecated:: 0.2.7 Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead. It will not be removed until langchain==1.0.\n",
      " |  \n",
      " |  Chain to have a conversation and load context from memory.\n",
      " |  \n",
      " |  This class is deprecated in favor of ``RunnableWithMessageHistory``. Please refer\n",
      " |  to this tutorial for more detail: https://python.langchain.com/docs/tutorials/chatbot/\n",
      " |  \n",
      " |  ``RunnableWithMessageHistory`` offers several benefits, including:\n",
      " |  \n",
      " |  - Stream, batch, and async support;\n",
      " |  - More flexible memory handling, including the ability to manage memory\n",
      " |    outside the chain;\n",
      " |  - Support for multiple threads.\n",
      " |  \n",
      " |  Below is a minimal implementation, analogous to using ``ConversationChain`` with\n",
      " |  the default ``ConversationBufferMemory``:\n",
      " |  \n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |          from langchain_core.chat_history import InMemoryChatMessageHistory\n",
      " |          from langchain_core.runnables.history import RunnableWithMessageHistory\n",
      " |          from langchain_openai import ChatOpenAI\n",
      " |  \n",
      " |  \n",
      " |          store = {}  # memory is maintained outside the chain\n",
      " |  \n",
      " |          def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
      " |              if session_id not in store:\n",
      " |                  store[session_id] = InMemoryChatMessageHistory()\n",
      " |              return store[session_id]\n",
      " |  \n",
      " |          llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
      " |  \n",
      " |          chain = RunnableWithMessageHistory(llm, get_session_history)\n",
      " |          chain.invoke(\n",
      " |              \"Hi I'm Bob.\",\n",
      " |              config={\"configurable\": {\"session_id\": \"1\"}},\n",
      " |          )  # session_id determines thread\n",
      " |  Memory objects can also be incorporated into the ``get_session_history`` callable:\n",
      " |  \n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |          from langchain.memory import ConversationBufferWindowMemory\n",
      " |          from langchain_core.chat_history import InMemoryChatMessageHistory\n",
      " |          from langchain_core.runnables.history import RunnableWithMessageHistory\n",
      " |          from langchain_openai import ChatOpenAI\n",
      " |  \n",
      " |  \n",
      " |          store = {}  # memory is maintained outside the chain\n",
      " |  \n",
      " |          def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
      " |              if session_id not in store:\n",
      " |                  store[session_id] = InMemoryChatMessageHistory()\n",
      " |                  return store[session_id]\n",
      " |  \n",
      " |              memory = ConversationBufferWindowMemory(\n",
      " |                  chat_memory=store[session_id],\n",
      " |                  k=3,\n",
      " |                  return_messages=True,\n",
      " |              )\n",
      " |              assert len(memory.memory_variables) == 1\n",
      " |              key = memory.memory_variables[0]\n",
      " |              messages = memory.load_memory_variables({})[key]\n",
      " |              store[session_id] = InMemoryChatMessageHistory(messages=messages)\n",
      " |              return store[session_id]\n",
      " |  \n",
      " |          llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
      " |  \n",
      " |          chain = RunnableWithMessageHistory(llm, get_session_history)\n",
      " |          chain.invoke(\n",
      " |              \"Hi I'm Bob.\",\n",
      " |              config={\"configurable\": {\"session_id\": \"1\"}},\n",
      " |          )  # session_id determines thread\n",
      " |  \n",
      " |  Example:\n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |          from langchain.chains import ConversationChain\n",
      " |          from langchain_community.llms import OpenAI\n",
      " |  \n",
      " |          conversation = ConversationChain(llm=OpenAI())\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ConversationChain\n",
      " |      langchain.chains.llm.LLMChain\n",
      " |      langchain.chains.base.Chain\n",
      " |      langchain_core.runnables.base.RunnableSerializable[dict[str, Any], dict[str, Any]]\n",
      " |      langchain_core.runnables.base.RunnableSerializable\n",
      " |      langchain_core.load.serializable.Serializable\n",
      " |      pydantic.main.BaseModel\n",
      " |      langchain_core.runnables.base.Runnable\n",
      " |      abc.ABC\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *args: Any, **kwargs: Any) -> None\n",
      " |  \n",
      " |  validate_prompt_input_variables(self) -> typing_extensions.Self\n",
      " |      Validate that prompt input variables are consistent.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  is_lc_serializable() -> bool from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Is this class serializable?\n",
      " |      \n",
      " |      By design, even if a class inherits from Serializable, it is not serializable by\n",
      " |      default. This is to prevent accidental serialization of objects that should not\n",
      " |      be serialized.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Whether the class is serializable. Default is False.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  input_keys\n",
      " |      Use this since so some prompt vars come from history.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'input_key': <class 'str'>, 'memory': <class 'langc...\n",
      " |  \n",
      " |  __class_vars__ = set()\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  __private_attributes__ = {}\n",
      " |  \n",
      " |  __pydantic_complete__ = True\n",
      " |  \n",
      " |  __pydantic_computed_fields__ = {}\n",
      " |  \n",
      " |  __pydantic_core_schema__ = {'function': {'function': <function Convers...\n",
      " |  \n",
      " |  __pydantic_custom_init__ = True\n",
      " |  \n",
      " |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
      " |  \n",
      " |  __pydantic_fields__ = {'callback_manager': FieldInfo(annotation=Union[...\n",
      " |  \n",
      " |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
      " |  \n",
      " |  __pydantic_parent_namespace__ = None\n",
      " |  \n",
      " |  __pydantic_post_init__ = None\n",
      " |  \n",
      " |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
      " |      Model...\n",
      " |  \n",
      " |  __pydantic_setattr_handlers__ = {}\n",
      " |  \n",
      " |  __pydantic_validator__ = SchemaValidator(title=\"ConversationChain\", va...\n",
      " |  \n",
      " |  __signature__ = <Signature (*args: Any, name: Optional[str] = No...t =...\n",
      " |  \n",
      " |  model_config = {'arbitrary_types_allowed': True, 'extra': 'forbid', 'p...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain.chains.llm.LLMChain:\n",
      " |  \n",
      " |  async aapply(self, input_list: 'list[dict[str, Any]]', callbacks: 'Callbacks' = None) -> 'list[dict[str, str]]'\n",
      " |      Utilize the LLM generate method for speed gains.\n",
      " |  \n",
      " |  async aapply_and_parse(self, input_list: 'list[dict[str, Any]]', callbacks: 'Callbacks' = None) -> 'Sequence[Union[str, list[str], dict[str, str]]]'\n",
      " |      Call apply and then parse the results.\n",
      " |  \n",
      " |  async agenerate(self, input_list: 'list[dict[str, Any]]', run_manager: 'Optional[AsyncCallbackManagerForChainRun]' = None) -> 'LLMResult'\n",
      " |      Generate LLM result from inputs.\n",
      " |  \n",
      " |  apply(self, input_list: 'list[dict[str, Any]]', callbacks: 'Callbacks' = None) -> 'list[dict[str, str]]'\n",
      " |      Utilize the LLM generate method for speed gains.\n",
      " |  \n",
      " |  apply_and_parse(self, input_list: 'list[dict[str, Any]]', callbacks: 'Callbacks' = None) -> 'Sequence[Union[str, list[str], dict[str, str]]]'\n",
      " |      Call apply and then parse the results.\n",
      " |  \n",
      " |  async apredict(self, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'str'\n",
      " |      Format prompt with kwargs and pass to LLM.\n",
      " |      \n",
      " |      Args:\n",
      " |          callbacks: Callbacks to pass to LLMChain\n",
      " |          **kwargs: Keys to pass to prompt template.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Completion from LLM.\n",
      " |      \n",
      " |      Example:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              completion = llm.predict(adjective=\"funny\")\n",
      " |  \n",
      " |  async apredict_and_parse(self, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'Union[str, list[str], dict[str, str]]'\n",
      " |      Call apredict and then parse the results.\n",
      " |  \n",
      " |  async aprep_prompts(self, input_list: 'list[dict[str, Any]]', run_manager: 'Optional[AsyncCallbackManagerForChainRun]' = None) -> 'tuple[list[PromptValue], Optional[list[str]]]'\n",
      " |      Prepare prompts from inputs.\n",
      " |  \n",
      " |  create_outputs(self, llm_result: 'LLMResult') -> 'list[dict[str, Any]]'\n",
      " |      Create outputs from response.\n",
      " |  \n",
      " |  generate(self, input_list: 'list[dict[str, Any]]', run_manager: 'Optional[CallbackManagerForChainRun]' = None) -> 'LLMResult'\n",
      " |      Generate LLM result from inputs.\n",
      " |  \n",
      " |  predict(self, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'str'\n",
      " |      Format prompt with kwargs and pass to LLM.\n",
      " |      \n",
      " |      Args:\n",
      " |          callbacks: Callbacks to pass to LLMChain\n",
      " |          **kwargs: Keys to pass to prompt template.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Completion from LLM.\n",
      " |      \n",
      " |      Example:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              completion = llm.predict(adjective=\"funny\")\n",
      " |  \n",
      " |  predict_and_parse(self, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'Union[str, list[str], dict[str, Any]]'\n",
      " |      Call predict and then parse the results.\n",
      " |  \n",
      " |  prep_prompts(self, input_list: 'list[dict[str, Any]]', run_manager: 'Optional[CallbackManagerForChainRun]' = None) -> 'tuple[list[PromptValue], Optional[list[str]]]'\n",
      " |      Prepare prompts from inputs.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain.chains.llm.LLMChain:\n",
      " |  \n",
      " |  from_string(llm: 'BaseLanguageModel', template: 'str') -> 'LLMChain' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Create LLMChain from LLM and template.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain.chains.llm.LLMChain:\n",
      " |  \n",
      " |  output_keys\n",
      " |      Will always return text key.\n",
      " |      \n",
      " |      :meta private:\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain.chains.base.Chain:\n",
      " |  \n",
      " |  __call__(self, inputs: Union[dict[str, Any], Any], return_only_outputs: bool = False, callbacks: Union[list[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, *, tags: Optional[list[str]] = None, metadata: Optional[dict[str, Any]] = None, run_name: Optional[str] = None, include_run_info: bool = False) -> dict\n",
      " |      .. deprecated:: 0.1.0 Use :meth:`~invoke` instead. It will not be removed until langchain==1.0.\n",
      " |      \n",
      " |      Execute the chain.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: Dictionary of inputs, or single input if chain expects\n",
      " |              only one param. Should contain all inputs specified in\n",
      " |              `Chain.input_keys` except for inputs that will be set by the chain's\n",
      " |              memory.\n",
      " |          return_only_outputs: Whether to return only outputs in the\n",
      " |              response. If True, only new keys generated by this chain will be\n",
      " |              returned. If False, both input keys and new keys generated by this\n",
      " |              chain will be returned. Defaults to False.\n",
      " |          callbacks: Callbacks to use for this chain run. These will be called in\n",
      " |              addition to callbacks passed to the chain during construction, but only\n",
      " |              these runtime callbacks will propagate to calls to other objects.\n",
      " |          tags: List of string tags to pass to all callbacks. These will be passed in\n",
      " |              addition to tags passed to the chain during construction, but only\n",
      " |              these runtime tags will propagate to calls to other objects.\n",
      " |          metadata: Optional metadata associated with the chain. Defaults to None\n",
      " |          include_run_info: Whether to include run info in the response. Defaults\n",
      " |              to False.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dict of named outputs. Should contain all outputs specified in\n",
      " |              `Chain.output_keys`.\n",
      " |  \n",
      " |  async acall(self, inputs: Union[dict[str, Any], Any], return_only_outputs: bool = False, callbacks: Union[list[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, *, tags: Optional[list[str]] = None, metadata: Optional[dict[str, Any]] = None, run_name: Optional[str] = None, include_run_info: bool = False) -> dict\n",
      " |      .. deprecated:: 0.1.0 Use :meth:`~ainvoke` instead. It will not be removed until langchain==1.0.\n",
      " |      \n",
      " |      Asynchronously execute the chain.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: Dictionary of inputs, or single input if chain expects\n",
      " |              only one param. Should contain all inputs specified in\n",
      " |              `Chain.input_keys` except for inputs that will be set by the chain's\n",
      " |              memory.\n",
      " |          return_only_outputs: Whether to return only outputs in the\n",
      " |              response. If True, only new keys generated by this chain will be\n",
      " |              returned. If False, both input keys and new keys generated by this\n",
      " |              chain will be returned. Defaults to False.\n",
      " |          callbacks: Callbacks to use for this chain run. These will be called in\n",
      " |              addition to callbacks passed to the chain during construction, but only\n",
      " |              these runtime callbacks will propagate to calls to other objects.\n",
      " |          tags: List of string tags to pass to all callbacks. These will be passed in\n",
      " |              addition to tags passed to the chain during construction, but only\n",
      " |              these runtime tags will propagate to calls to other objects.\n",
      " |          metadata: Optional metadata associated with the chain. Defaults to None\n",
      " |          include_run_info: Whether to include run info in the response. Defaults\n",
      " |              to False.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dict of named outputs. Should contain all outputs specified in\n",
      " |              `Chain.output_keys`.\n",
      " |  \n",
      " |  async ainvoke(self, input: dict, config: Optional[langchain_core.runnables.config.RunnableConfig] = None, **kwargs: Any) -> dict\n",
      " |      Transform a single input into an output.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The input to the ``Runnable``.\n",
      " |          config: A config to use when invoking the ``Runnable``.\n",
      " |              The config supports standard keys like ``'tags'``, ``'metadata'`` for\n",
      " |              tracing purposes, ``'max_concurrency'`` for controlling how much work to\n",
      " |              do in parallel, and other keys. Please refer to the ``RunnableConfig``\n",
      " |              for more details. Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The output of the ``Runnable``.\n",
      " |  \n",
      " |  async aprep_inputs(self, inputs: Union[dict[str, Any], Any]) -> dict\n",
      " |      Prepare chain inputs, including adding inputs from memory.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: Dictionary of raw inputs, or single input if chain expects\n",
      " |              only one param. Should contain all inputs specified in\n",
      " |              `Chain.input_keys` except for inputs that will be set by the chain's\n",
      " |              memory.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dictionary of all inputs, including those added by the chain's memory.\n",
      " |  \n",
      " |  async aprep_outputs(self, inputs: dict, outputs: dict, return_only_outputs: bool = False) -> dict\n",
      " |      Validate and prepare chain outputs, and save info about this run to memory.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: Dictionary of chain inputs, including any inputs added by chain\n",
      " |              memory.\n",
      " |          outputs: Dictionary of initial chain outputs.\n",
      " |          return_only_outputs: Whether to only return the chain outputs. If False,\n",
      " |              inputs are also added to the final outputs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dict of the final chain outputs.\n",
      " |  \n",
      " |  async arun(self, *args: Any, callbacks: Union[list[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, tags: Optional[list[str]] = None, metadata: Optional[dict[str, Any]] = None, **kwargs: Any) -> Any\n",
      " |      .. deprecated:: 0.1.0 Use :meth:`~ainvoke` instead. It will not be removed until langchain==1.0.\n",
      " |      \n",
      " |      Convenience method for executing chain.\n",
      " |      \n",
      " |      The main difference between this method and `Chain.__call__` is that this\n",
      " |      method expects inputs to be passed directly in as positional arguments or\n",
      " |      keyword arguments, whereas `Chain.__call__` expects a single input dictionary\n",
      " |      with all the inputs\n",
      " |      \n",
      " |      \n",
      " |      Args:\n",
      " |          *args: If the chain expects a single input, it can be passed in as the\n",
      " |              sole positional argument.\n",
      " |          callbacks: Callbacks to use for this chain run. These will be called in\n",
      " |              addition to callbacks passed to the chain during construction, but only\n",
      " |              these runtime callbacks will propagate to calls to other objects.\n",
      " |          tags: List of string tags to pass to all callbacks. These will be passed in\n",
      " |              addition to tags passed to the chain during construction, but only\n",
      " |              these runtime tags will propagate to calls to other objects.\n",
      " |          **kwargs: If the chain expects multiple inputs, they can be passed in\n",
      " |              directly as keyword arguments.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The chain output.\n",
      " |      \n",
      " |      Example:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              # Suppose we have a single-input chain that takes a 'question' string:\n",
      " |              await chain.arun(\"What's the temperature in Boise, Idaho?\")\n",
      " |              # -> \"The temperature in Boise is...\"\n",
      " |      \n",
      " |              # Suppose we have a multi-input chain that takes a 'question' string\n",
      " |              # and 'context' string:\n",
      " |              question = \"What's the temperature in Boise, Idaho?\"\n",
      " |              context = \"Weather report for Boise, Idaho on 07/03/23...\"\n",
      " |              await chain.arun(question=question, context=context)\n",
      " |              # -> \"The temperature in Boise is...\"\n",
      " |  \n",
      " |  dict(self, **kwargs: Any) -> dict\n",
      " |      Dictionary representation of chain.\n",
      " |      \n",
      " |      Expects `Chain._chain_type` property to be implemented and for memory to be\n",
      " |          null.\n",
      " |      \n",
      " |      Args:\n",
      " |          **kwargs: Keyword arguments passed to default `pydantic.BaseModel.dict`\n",
      " |              method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dictionary representation of the chain.\n",
      " |      \n",
      " |      Example:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              chain.dict(exclude_unset=True)\n",
      " |              # -> {\"_type\": \"foo\", \"verbose\": False, ...}\n",
      " |  \n",
      " |  get_input_schema(self, config: Optional[langchain_core.runnables.config.RunnableConfig] = None) -> type\n",
      " |      Get a pydantic model that can be used to validate input to the Runnable.\n",
      " |      \n",
      " |      ``Runnable``s that leverage the ``configurable_fields`` and\n",
      " |      ``configurable_alternatives`` methods will have a dynamic input schema that\n",
      " |      depends on which configuration the ``Runnable`` is invoked with.\n",
      " |      \n",
      " |      This method allows to get an input schema for a specific configuration.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate input.\n",
      " |  \n",
      " |  get_output_schema(self, config: Optional[langchain_core.runnables.config.RunnableConfig] = None) -> type\n",
      " |      Get a pydantic model that can be used to validate output to the ``Runnable``.\n",
      " |      \n",
      " |      ``Runnable``s that leverage the ``configurable_fields`` and\n",
      " |      ``configurable_alternatives`` methods will have a dynamic output schema that\n",
      " |      depends on which configuration the ``Runnable`` is invoked with.\n",
      " |      \n",
      " |      This method allows to get an output schema for a specific configuration.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate output.\n",
      " |  \n",
      " |  invoke(self, input: dict, config: Optional[langchain_core.runnables.config.RunnableConfig] = None, **kwargs: Any) -> dict\n",
      " |      Transform a single input into an output.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The input to the ``Runnable``.\n",
      " |          config: A config to use when invoking the ``Runnable``.\n",
      " |              The config supports standard keys like ``'tags'``, ``'metadata'`` for\n",
      " |              tracing purposes, ``'max_concurrency'`` for controlling how much work to\n",
      " |              do in parallel, and other keys. Please refer to the ``RunnableConfig``\n",
      " |              for more details. Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The output of the ``Runnable``.\n",
      " |  \n",
      " |  prep_inputs(self, inputs: Union[dict[str, Any], Any]) -> dict\n",
      " |      Prepare chain inputs, including adding inputs from memory.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: Dictionary of raw inputs, or single input if chain expects\n",
      " |              only one param. Should contain all inputs specified in\n",
      " |              `Chain.input_keys` except for inputs that will be set by the chain's\n",
      " |              memory.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dictionary of all inputs, including those added by the chain's memory.\n",
      " |  \n",
      " |  prep_outputs(self, inputs: dict, outputs: dict, return_only_outputs: bool = False) -> dict\n",
      " |      Validate and prepare chain outputs, and save info about this run to memory.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: Dictionary of chain inputs, including any inputs added by chain\n",
      " |              memory.\n",
      " |          outputs: Dictionary of initial chain outputs.\n",
      " |          return_only_outputs: Whether to only return the chain outputs. If False,\n",
      " |              inputs are also added to the final outputs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dict of the final chain outputs.\n",
      " |  \n",
      " |  run(self, *args: Any, callbacks: Union[list[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, tags: Optional[list[str]] = None, metadata: Optional[dict[str, Any]] = None, **kwargs: Any) -> Any\n",
      " |      .. deprecated:: 0.1.0 Use :meth:`~invoke` instead. It will not be removed until langchain==1.0.\n",
      " |      \n",
      " |      Convenience method for executing chain.\n",
      " |      \n",
      " |      The main difference between this method and `Chain.__call__` is that this\n",
      " |      method expects inputs to be passed directly in as positional arguments or\n",
      " |      keyword arguments, whereas `Chain.__call__` expects a single input dictionary\n",
      " |      with all the inputs\n",
      " |      \n",
      " |      Args:\n",
      " |          *args: If the chain expects a single input, it can be passed in as the\n",
      " |              sole positional argument.\n",
      " |          callbacks: Callbacks to use for this chain run. These will be called in\n",
      " |              addition to callbacks passed to the chain during construction, but only\n",
      " |              these runtime callbacks will propagate to calls to other objects.\n",
      " |          tags: List of string tags to pass to all callbacks. These will be passed in\n",
      " |              addition to tags passed to the chain during construction, but only\n",
      " |              these runtime tags will propagate to calls to other objects.\n",
      " |          **kwargs: If the chain expects multiple inputs, they can be passed in\n",
      " |              directly as keyword arguments.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The chain output.\n",
      " |      \n",
      " |      Example:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              # Suppose we have a single-input chain that takes a 'question' string:\n",
      " |              chain.run(\"What's the temperature in Boise, Idaho?\")\n",
      " |              # -> \"The temperature in Boise is...\"\n",
      " |      \n",
      " |              # Suppose we have a multi-input chain that takes a 'question' string\n",
      " |              # and 'context' string:\n",
      " |              question = \"What's the temperature in Boise, Idaho?\"\n",
      " |              context = \"Weather report for Boise, Idaho on 07/03/23...\"\n",
      " |              chain.run(question=question, context=context)\n",
      " |              # -> \"The temperature in Boise is...\"\n",
      " |  \n",
      " |  save(self, file_path: Union[pathlib.Path, str]) -> None\n",
      " |      Save the chain.\n",
      " |      \n",
      " |      Expects `Chain._chain_type` property to be implemented and for memory to be\n",
      " |          null.\n",
      " |      \n",
      " |      Args:\n",
      " |          file_path: Path to file to save the chain to.\n",
      " |      \n",
      " |      Example:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              chain.save(file_path=\"path/chain.yaml\")\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain.chains.base.Chain:\n",
      " |  \n",
      " |  raise_callback_manager_deprecation(values: dict) -> Any from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Raise deprecation warning if callback_manager is used.\n",
      " |  \n",
      " |  set_verbose(verbose: Optional[bool]) -> bool from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Set the chain verbosity.\n",
      " |      \n",
      " |      Defaults to the global setting if not specified by the user.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.runnables.base.RunnableSerializable:\n",
      " |  \n",
      " |  configurable_alternatives(self, which: 'ConfigurableField', *, default_key: 'str' = 'default', prefix_keys: 'bool' = False, **kwargs: 'Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]]') -> 'RunnableSerializable[Input, Output]'\n",
      " |      Configure alternatives for ``Runnables`` that can be set at runtime.\n",
      " |      \n",
      " |      Args:\n",
      " |          which: The ``ConfigurableField`` instance that will be used to select the\n",
      " |              alternative.\n",
      " |          default_key: The default key to use if no alternative is selected.\n",
      " |              Defaults to ``'default'``.\n",
      " |          prefix_keys: Whether to prefix the keys with the ``ConfigurableField`` id.\n",
      " |              Defaults to False.\n",
      " |          **kwargs: A dictionary of keys to ``Runnable`` instances or callables that\n",
      " |              return ``Runnable`` instances.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new ``Runnable`` with the alternatives configured.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_anthropic import ChatAnthropic\n",
      " |          from langchain_core.runnables.utils import ConfigurableField\n",
      " |          from langchain_openai import ChatOpenAI\n",
      " |      \n",
      " |          model = ChatAnthropic(\n",
      " |              model_name=\"claude-3-7-sonnet-20250219\"\n",
      " |          ).configurable_alternatives(\n",
      " |              ConfigurableField(id=\"llm\"),\n",
      " |              default_key=\"anthropic\",\n",
      " |              openai=ChatOpenAI(),\n",
      " |          )\n",
      " |      \n",
      " |          # uses the default model ChatAnthropic\n",
      " |          print(model.invoke(\"which organization created you?\").content)\n",
      " |      \n",
      " |          # uses ChatOpenAI\n",
      " |          print(\n",
      " |              model.with_config(configurable={\"llm\": \"openai\"})\n",
      " |              .invoke(\"which organization created you?\")\n",
      " |              .content\n",
      " |          )\n",
      " |  \n",
      " |  configurable_fields(self, **kwargs: 'AnyConfigurableField') -> 'RunnableSerializable[Input, Output]'\n",
      " |      Configure particular ``Runnable`` fields at runtime.\n",
      " |      \n",
      " |      Args:\n",
      " |          **kwargs: A dictionary of ``ConfigurableField`` instances to configure.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If a configuration key is not found in the ``Runnable``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new ``Runnable`` with the fields configured.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_core.runnables import ConfigurableField\n",
      " |          from langchain_openai import ChatOpenAI\n",
      " |      \n",
      " |          model = ChatOpenAI(max_tokens=20).configurable_fields(\n",
      " |              max_tokens=ConfigurableField(\n",
      " |                  id=\"output_token_number\",\n",
      " |                  name=\"Max tokens in the output\",\n",
      " |                  description=\"The maximum number of tokens in the output\",\n",
      " |              )\n",
      " |          )\n",
      " |      \n",
      " |          # max_tokens = 20\n",
      " |          print(\n",
      " |              \"max_tokens_20: \", model.invoke(\"tell me something about chess\").content\n",
      " |          )\n",
      " |      \n",
      " |          # max_tokens = 200\n",
      " |          print(\n",
      " |              \"max_tokens_200: \",\n",
      " |              model.with_config(configurable={\"output_token_number\": 200})\n",
      " |              .invoke(\"tell me something about chess\")\n",
      " |              .content,\n",
      " |          )\n",
      " |  \n",
      " |  to_json(self) -> 'Union[SerializedConstructor, SerializedNotImplemented]'\n",
      " |      Serialize the ``Runnable`` to JSON.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A JSON-serializable representation of the ``Runnable``.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from langchain_core.runnables.base.RunnableSerializable:\n",
      " |  \n",
      " |  __orig_bases__ = (<class 'langchain_core.load.serializable.Serializabl...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.load.serializable.Serializable:\n",
      " |  \n",
      " |  __repr_args__(self) -> Any\n",
      " |  \n",
      " |  to_json_not_implemented(self) -> langchain_core.load.serializable.SerializedNotImplemented\n",
      " |      Serialize a \"not implemented\" object.\n",
      " |      \n",
      " |      Returns:\n",
      " |          SerializedNotImplemented.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.load.serializable.Serializable:\n",
      " |  \n",
      " |  get_lc_namespace() -> list from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Get the namespace of the langchain object.\n",
      " |      \n",
      " |      For example, if the class is `langchain.llms.openai.OpenAI`, then the\n",
      " |      namespace is [\"langchain\", \"llms\", \"openai\"]\n",
      " |      \n",
      " |      Returns:\n",
      " |          The namespace as a list of strings.\n",
      " |  \n",
      " |  lc_id() -> list from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Return a unique identifier for this class for serialization purposes.\n",
      " |      \n",
      " |      The unique identifier is a list of strings that describes the path\n",
      " |      to the object.\n",
      " |      For example, for the class `langchain.llms.openai.OpenAI`, the id is\n",
      " |      [\"langchain\", \"llms\", \"openai\", \"OpenAI\"].\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.load.serializable.Serializable:\n",
      " |  \n",
      " |  lc_attributes\n",
      " |      List of attribute names that should be included in the serialized kwargs.\n",
      " |      \n",
      " |      These attributes must be accepted by the constructor.\n",
      " |      Default is an empty dictionary.\n",
      " |  \n",
      " |  lc_secrets\n",
      " |      A map of constructor argument names to secret ids.\n",
      " |      \n",
      " |      For example,\n",
      " |          {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from langchain_core.load.serializable.Serializable:\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __copy__(self) -> 'Self'\n",
      " |      Returns a shallow copy of the model.\n",
      " |  \n",
      " |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
      " |      Returns a deep copy of the model.\n",
      " |  \n",
      " |  __delattr__(self, item: 'str') -> 'Any'\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __eq__(self, other: 'Any') -> 'bool'\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __getattr__(self, item: 'str') -> 'Any'\n",
      " |  \n",
      " |  __getstate__(self) -> 'dict[Any, Any]'\n",
      " |  \n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      So `dict(model)` works.\n",
      " |  \n",
      " |  __pretty__(self, fmt: 'Callable[[Any], Any]', **kwargs: 'Any') -> 'Generator[Any]'\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
      " |  \n",
      " |  __replace__(self, **changes: 'Any') -> 'Self'\n",
      " |      # Because we make use of `@dataclass_transform()`, `__replace__` is already synthesized by\n",
      " |      # type checkers, so we define the implementation in this `if not TYPE_CHECKING:` block:\n",
      " |  \n",
      " |  __repr__(self) -> 'str'\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __repr_name__(self) -> 'str'\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |  \n",
      " |  __repr_recursion__(self, object: 'Any') -> 'str'\n",
      " |      Returns the string representation of a recursive object.\n",
      " |  \n",
      " |  __repr_str__(self, join_str: 'str') -> 'str'\n",
      " |  \n",
      " |  __rich_repr__(self) -> 'RichReprResult'\n",
      " |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
      " |  \n",
      " |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
      " |  \n",
      " |  __str__(self) -> 'str'\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  copy(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      " |      Returns a copy of the model.\n",
      " |      \n",
      " |      !!! warning \"Deprecated\"\n",
      " |          This method is now deprecated; use `model_copy` instead.\n",
      " |      \n",
      " |      If you need `include` or `exclude`, use:\n",
      " |      \n",
      " |      ```python {test=\"skip\" lint=\"skip\"}\n",
      " |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
      " |      data = {**data, **(update or {})}\n",
      " |      copied = self.model_validate(data)\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
      " |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
      " |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
      " |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A copy of the model with included, excluded and updated fields as specified.\n",
      " |  \n",
      " |  json(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -> 'str'\n",
      " |  \n",
      " |  model_copy(self, *, update: 'Mapping[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [`model_copy`](../concepts/models.md#model-copy)\n",
      " |      \n",
      " |      Returns a copy of the model.\n",
      " |      \n",
      " |      !!! note\n",
      " |          The underlying instance's [`__dict__`][object.__dict__] attribute is copied. This\n",
      " |          might have unexpected side effects if you store anything in it, on top of the model\n",
      " |          fields (e.g. the value of [cached properties][functools.cached_property]).\n",
      " |      \n",
      " |      Args:\n",
      " |          update: Values to change/add in the new model. Note: the data is not validated\n",
      " |              before creating the new model. You should trust this data.\n",
      " |          deep: Set to `True` to make a deep copy of the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          New model instance.\n",
      " |  \n",
      " |  model_dump(self, *, mode: \"Literal['json', 'python'] | str\" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, exclude_computed_fields: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, fallback: 'Callable[[Any], Any] | None' = None, serialize_as_any: 'bool' = False) -> 'dict[str, Any]'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [`model_dump`](../concepts/serialization.md#python-mode)\n",
      " |      \n",
      " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode: The mode in which `to_python` should run.\n",
      " |              If mode is 'json', the output will only contain JSON serializable types.\n",
      " |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
      " |          include: A set of fields to include in the output.\n",
      " |          exclude: A set of fields to exclude from the output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          exclude_computed_fields: Whether to exclude computed fields.\n",
      " |              While this can be useful for round-tripping, it is usually recommended to use the dedicated\n",
      " |              `round_trip` parameter instead.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          fallback: A function to call when an unknown value is encountered. If not provided,\n",
      " |              a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dictionary representation of the model.\n",
      " |  \n",
      " |  model_dump_json(self, *, indent: 'int | None' = None, ensure_ascii: 'bool' = False, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, exclude_computed_fields: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, fallback: 'Callable[[Any], Any] | None' = None, serialize_as_any: 'bool' = False) -> 'str'\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [`model_dump_json`](../concepts/serialization.md#json-mode)\n",
      " |      \n",
      " |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
      " |      \n",
      " |      Args:\n",
      " |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
      " |          ensure_ascii: If `True`, the output is guaranteed to have all incoming non-ASCII characters escaped.\n",
      " |              If `False` (the default), these characters will be output as-is.\n",
      " |          include: Field(s) to include in the JSON output.\n",
      " |          exclude: Field(s) to exclude from the JSON output.\n",
      " |          context: Additional context to pass to the serializer.\n",
      " |          by_alias: Whether to serialize using field aliases.\n",
      " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
      " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
      " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
      " |          exclude_computed_fields: Whether to exclude computed fields.\n",
      " |              While this can be useful for round-tripping, it is usually recommended to use the dedicated\n",
      " |              `round_trip` parameter instead.\n",
      " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
      " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
      " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
      " |          fallback: A function to call when an unknown value is encountered. If not provided,\n",
      " |              a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.\n",
      " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A JSON string representation of the model.\n",
      " |  \n",
      " |  model_post_init(self, context: 'Any', /) -> 'None'\n",
      " |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
      " |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |  \n",
      " |  __get_pydantic_core_schema__(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -> 'CoreSchema' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |  \n",
      " |  __get_pydantic_json_schema__(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -> 'JsonSchemaValue' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Hook into generating the model's JSON schema.\n",
      " |      \n",
      " |      Args:\n",
      " |          core_schema: A `pydantic-core` CoreSchema.\n",
      " |              You can ignore this argument and call the handler with a new CoreSchema,\n",
      " |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
      " |              or just call the handler with the original schema.\n",
      " |          handler: Call into Pydantic's internal JSON schema generation.\n",
      " |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
      " |              generation fails.\n",
      " |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
      " |              `schema_generator` argument to that function to change JSON schema generation globally\n",
      " |              for a type.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A JSON schema, as a Python object.\n",
      " |  \n",
      " |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
      " |      only after basic class initialization is complete. In particular, attributes like `model_fields` will\n",
      " |      be present when this is called, but forward annotations are not guaranteed to be resolved yet,\n",
      " |      meaning that creating an instance of the class may fail.\n",
      " |      \n",
      " |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
      " |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
      " |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
      " |      \n",
      " |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
      " |      any kwargs passed to the class definition that aren't used internally by Pydantic.\n",
      " |      \n",
      " |      Args:\n",
      " |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
      " |              by Pydantic.\n",
      " |      \n",
      " |      Note:\n",
      " |          You may want to override [`__pydantic_on_complete__()`][pydantic.main.BaseModel.__pydantic_on_complete__]\n",
      " |          instead, which is called once the class and its fields are fully initialized and ready for validation.\n",
      " |  \n",
      " |  __pydantic_on_complete__() -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      This is called once the class and its fields are fully initialized and ready to be used.\n",
      " |      \n",
      " |      This typically happens when the class is created (just before\n",
      " |      [`__pydantic_init_subclass__()`][pydantic.main.BaseModel.__pydantic_init_subclass__] is called on the superclass),\n",
      " |      except when forward annotations are used that could not immediately be resolved.\n",
      " |      In that case, it will be called later, when the model is rebuilt automatically or explicitly using\n",
      " |      [`model_rebuild()`][pydantic.main.BaseModel.model_rebuild].\n",
      " |  \n",
      " |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |  \n",
      " |  from_orm(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |  \n",
      " |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Creates a new instance of the `Model` class with validated data.\n",
      " |      \n",
      " |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |      \n",
      " |      !!! note\n",
      " |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
      " |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
      " |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
      " |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
      " |          an error if extra values are passed, but they will be ignored.\n",
      " |      \n",
      " |      Args:\n",
      " |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
      " |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
      " |              Otherwise, the field names from the `values` argument will be used.\n",
      " |          values: Trusted or pre-validated data dictionary.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new instance of the `Model` class with validated data.\n",
      " |  \n",
      " |  model_json_schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: 'JsonSchemaMode' = 'validation', *, union_format: \"Literal['any_of', 'primitive_type_array']\" = 'any_of') -> 'dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Generates a JSON schema for a model class.\n",
      " |      \n",
      " |      Args:\n",
      " |          by_alias: Whether to use attribute aliases or not.\n",
      " |          ref_template: The reference template.\n",
      " |          union_format: The format to use when combining schemas from unions together. Can be one of:\n",
      " |      \n",
      " |              - `'any_of'`: Use the [`anyOf`](https://json-schema.org/understanding-json-schema/reference/combining#anyOf)\n",
      " |              keyword to combine schemas (the default).\n",
      " |              - `'primitive_type_array'`: Use the [`type`](https://json-schema.org/understanding-json-schema/reference/type)\n",
      " |              keyword as an array of strings, containing each type of the combination. If any of the schemas is not a primitive\n",
      " |              type (`string`, `boolean`, `null`, `integer` or `number`) or contains constraints/metadata, falls back to\n",
      " |              `any_of`.\n",
      " |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
      " |              `GenerateJsonSchema` with your desired modifications\n",
      " |          mode: The mode in which to generate the schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The JSON schema for the given model class.\n",
      " |  \n",
      " |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Compute the class name for parametrizations of generic classes.\n",
      " |      \n",
      " |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
      " |      \n",
      " |      Args:\n",
      " |          params: Tuple of types of the class. Given a generic class\n",
      " |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
      " |              the value `(str, int)` would be passed to `params`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          String representing the new class where `params` are passed to `cls` as type variables.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
      " |  \n",
      " |  model_rebuild(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'MappingNamespace | None' = None) -> 'bool | None' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Try to rebuild the pydantic-core schema for the model.\n",
      " |      \n",
      " |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
      " |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
      " |      \n",
      " |      Args:\n",
      " |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
      " |          raise_errors: Whether to raise errors, defaults to `True`.\n",
      " |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
      " |          _types_namespace: The types namespace, defaults to `None`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
      " |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
      " |  \n",
      " |  model_validate(obj: 'Any', *, strict: 'bool | None' = None, extra: 'ExtraValues | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, by_name: 'bool | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Validate a pydantic model instance.\n",
      " |      \n",
      " |      Args:\n",
      " |          obj: The object to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          extra: Whether to ignore, allow, or forbid extra data during model validation.\n",
      " |              See the [`extra` configuration value][pydantic.ConfigDict.extra] for details.\n",
      " |          from_attributes: Whether to extract data from object attributes.\n",
      " |          context: Additional context to pass to the validator.\n",
      " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
      " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValidationError: If the object could not be validated.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The validated model instance.\n",
      " |  \n",
      " |  model_validate_json(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, extra: 'ExtraValues | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, by_name: 'bool | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      !!! abstract \"Usage Documentation\"\n",
      " |          [JSON Parsing](../concepts/json.md#json-parsing)\n",
      " |      \n",
      " |      Validate the given JSON data against the Pydantic model.\n",
      " |      \n",
      " |      Args:\n",
      " |          json_data: The JSON data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          extra: Whether to ignore, allow, or forbid extra data during model validation.\n",
      " |              See the [`extra` configuration value][pydantic.ConfigDict.extra] for details.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
      " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
      " |  \n",
      " |  model_validate_strings(obj: 'Any', *, strict: 'bool | None' = None, extra: 'ExtraValues | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, by_name: 'bool | None' = None) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      Validate the given object with string data against the Pydantic model.\n",
      " |      \n",
      " |      Args:\n",
      " |          obj: The object containing string data to validate.\n",
      " |          strict: Whether to enforce types strictly.\n",
      " |          extra: Whether to ignore, allow, or forbid extra data during model validation.\n",
      " |              See the [`extra` configuration value][pydantic.ConfigDict.extra] for details.\n",
      " |          context: Extra variables to pass to the validator.\n",
      " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
      " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The validated Pydantic model.\n",
      " |  \n",
      " |  parse_file(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |  \n",
      " |  parse_obj(obj: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |  \n",
      " |  parse_raw(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |  \n",
      " |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |  \n",
      " |  schema_json(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -> 'str' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |  \n",
      " |  update_forward_refs(**localns: 'Any') -> 'None' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |  \n",
      " |  validate(value: 'Any') -> 'Self' from pydantic._internal._model_construction.ModelMetaclass\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __fields_set__\n",
      " |  \n",
      " |  model_extra\n",
      " |      Get extra fields set during validation.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
      " |  \n",
      " |  model_fields_set\n",
      " |      Returns the set of fields that have been explicitly set on this model instance.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A set of strings representing the fields that have been set,\n",
      " |              i.e. that were not filled from defaults.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __pydantic_extra__\n",
      " |  \n",
      " |  __pydantic_fields_set__\n",
      " |  \n",
      " |  __pydantic_private__\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __pydantic_root_model__ = False\n",
      " |  \n",
      " |  model_computed_fields = {}\n",
      " |  \n",
      " |  model_fields = {'callback_manager': FieldInfo(annotation=Union[BaseCal...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.runnables.base.Runnable:\n",
      " |  \n",
      " |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Iterator[Any]], Iterator[Other]], Callable[[AsyncIterator[Any]], AsyncIterator[Other]], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSerializable[Input, Other]'\n",
      " |      Runnable \"or\" operator.\n",
      " |      \n",
      " |      Compose this ``Runnable`` with another object to create a\n",
      " |      ``RunnableSequence``.\n",
      " |      \n",
      " |      Args:\n",
      " |          other: Another ``Runnable`` or a ``Runnable``-like object.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new ``Runnable``.\n",
      " |  \n",
      " |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Iterator[Other]], Iterator[Any]], Callable[[AsyncIterator[Other]], AsyncIterator[Any]], Callable[[Other], Any], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSerializable[Other, Output]'\n",
      " |      Runnable \"reverse-or\" operator.\n",
      " |      \n",
      " |      Compose this ``Runnable`` with another object to create a\n",
      " |      ``RunnableSequence``.\n",
      " |      \n",
      " |      Args:\n",
      " |          other: Another ``Runnable`` or a ``Runnable``-like object.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new ``Runnable``.\n",
      " |  \n",
      " |  async abatch(self, inputs: 'list[Input]', config: 'Optional[Union[RunnableConfig, list[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'list[Output]'\n",
      " |      Default implementation runs ``ainvoke`` in parallel using ``asyncio.gather``.\n",
      " |      \n",
      " |      The default implementation of ``batch`` works well for IO bound runnables.\n",
      " |      \n",
      " |      Subclasses should override this method if they can batch more efficiently;\n",
      " |      e.g., if the underlying ``Runnable`` uses an API which supports a batch mode.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: A list of inputs to the ``Runnable``.\n",
      " |          config: A config to use when invoking the ``Runnable``.\n",
      " |              The config supports standard keys like ``'tags'``, ``'metadata'`` for\n",
      " |              tracing purposes, ``'max_concurrency'`` for controlling how much work to\n",
      " |              do in parallel, and other keys. Please refer to the ``RunnableConfig``\n",
      " |              for more details. Defaults to None.\n",
      " |          return_exceptions: Whether to return exceptions instead of raising them.\n",
      " |              Defaults to False.\n",
      " |          **kwargs: Additional keyword arguments to pass to the ``Runnable``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of outputs from the ``Runnable``.\n",
      " |  \n",
      " |  async abatch_as_completed(self, inputs: 'Sequence[Input]', config: 'Optional[Union[RunnableConfig, Sequence[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'AsyncIterator[tuple[int, Union[Output, Exception]]]'\n",
      " |      Run ``ainvoke`` in parallel on a list of inputs.\n",
      " |      \n",
      " |      Yields results as they complete.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: A list of inputs to the ``Runnable``.\n",
      " |          config: A config to use when invoking the ``Runnable``.\n",
      " |              The config supports standard keys like ``'tags'``, ``'metadata'`` for\n",
      " |              tracing purposes, ``'max_concurrency'`` for controlling how much work to\n",
      " |              do in parallel, and other keys. Please refer to the ``RunnableConfig``\n",
      " |              for more details. Defaults to None.\n",
      " |          return_exceptions: Whether to return exceptions instead of raising them.\n",
      " |              Defaults to False.\n",
      " |          kwargs: Additional keyword arguments to pass to the ``Runnable``.\n",
      " |      \n",
      " |      Yields:\n",
      " |          A tuple of the index of the input and the output from the ``Runnable``.\n",
      " |  \n",
      " |  as_tool(self, args_schema: 'Optional[type[BaseModel]]' = None, *, name: 'Optional[str]' = None, description: 'Optional[str]' = None, arg_types: 'Optional[dict[str, type]]' = None) -> 'BaseTool'\n",
      " |      .. beta::\n",
      " |         This API is in beta and may change in the future.\n",
      " |      \n",
      " |      Create a ``BaseTool`` from a ``Runnable``.\n",
      " |      \n",
      " |      ``as_tool`` will instantiate a ``BaseTool`` with a name, description, and\n",
      " |      ``args_schema`` from a ``Runnable``. Where possible, schemas are inferred\n",
      " |      from ``runnable.get_input_schema``. Alternatively (e.g., if the\n",
      " |      ``Runnable`` takes a dict as input and the specific dict keys are not typed),\n",
      " |      the schema can be specified directly with ``args_schema``. You can also\n",
      " |      pass ``arg_types`` to just specify the required arguments and their types.\n",
      " |      \n",
      " |      Args:\n",
      " |          args_schema: The schema for the tool. Defaults to None.\n",
      " |          name: The name of the tool. Defaults to None.\n",
      " |          description: The description of the tool. Defaults to None.\n",
      " |          arg_types: A dictionary of argument names to types. Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A ``BaseTool`` instance.\n",
      " |      \n",
      " |      Typed dict input:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from typing_extensions import TypedDict\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |      \n",
      " |          class Args(TypedDict):\n",
      " |              a: int\n",
      " |              b: list[int]\n",
      " |      \n",
      " |      \n",
      " |          def f(x: Args) -> str:\n",
      " |              return str(x[\"a\"] * max(x[\"b\"]))\n",
      " |      \n",
      " |      \n",
      " |          runnable = RunnableLambda(f)\n",
      " |          as_tool = runnable.as_tool()\n",
      " |          as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n",
      " |      \n",
      " |      ``dict`` input, specifying schema via ``args_schema``:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from typing import Any\n",
      " |          from pydantic import BaseModel, Field\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |          def f(x: dict[str, Any]) -> str:\n",
      " |              return str(x[\"a\"] * max(x[\"b\"]))\n",
      " |      \n",
      " |          class FSchema(BaseModel):\n",
      " |              \"\"\"Apply a function to an integer and list of integers.\"\"\"\n",
      " |      \n",
      " |              a: int = Field(..., description=\"Integer\")\n",
      " |              b: list[int] = Field(..., description=\"List of ints\")\n",
      " |      \n",
      " |          runnable = RunnableLambda(f)\n",
      " |          as_tool = runnable.as_tool(FSchema)\n",
      " |          as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n",
      " |      \n",
      " |      ``dict`` input, specifying schema via ``arg_types``:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from typing import Any\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |      \n",
      " |          def f(x: dict[str, Any]) -> str:\n",
      " |              return str(x[\"a\"] * max(x[\"b\"]))\n",
      " |      \n",
      " |      \n",
      " |          runnable = RunnableLambda(f)\n",
      " |          as_tool = runnable.as_tool(arg_types={\"a\": int, \"b\": list[int]})\n",
      " |          as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n",
      " |      \n",
      " |      String input:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |      \n",
      " |          def f(x: str) -> str:\n",
      " |              return x + \"a\"\n",
      " |      \n",
      " |      \n",
      " |          def g(x: str) -> str:\n",
      " |              return x + \"z\"\n",
      " |      \n",
      " |      \n",
      " |          runnable = RunnableLambda(f) | g\n",
      " |          as_tool = runnable.as_tool()\n",
      " |          as_tool.invoke(\"b\")\n",
      " |      \n",
      " |      .. versionadded:: 0.2.14\n",
      " |  \n",
      " |  assign(self, **kwargs: 'Union[Runnable[dict[str, Any], Any], Callable[[dict[str, Any]], Any], Mapping[str, Union[Runnable[dict[str, Any], Any], Callable[[dict[str, Any]], Any]]]]') -> 'RunnableSerializable[Any, Any]'\n",
      " |      Assigns new fields to the dict output of this ``Runnable``.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_community.llms.fake import FakeStreamingListLLM\n",
      " |          from langchain_core.output_parsers import StrOutputParser\n",
      " |          from langchain_core.prompts import SystemMessagePromptTemplate\n",
      " |          from langchain_core.runnables import Runnable\n",
      " |          from operator import itemgetter\n",
      " |      \n",
      " |          prompt = (\n",
      " |              SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n",
      " |              + \"{question}\"\n",
      " |          )\n",
      " |          llm = FakeStreamingListLLM(responses=[\"foo-lish\"])\n",
      " |      \n",
      " |          chain: Runnable = prompt | llm | {\"str\": StrOutputParser()}\n",
      " |      \n",
      " |          chain_with_assign = chain.assign(hello=itemgetter(\"str\") | llm)\n",
      " |      \n",
      " |          print(chain_with_assign.input_schema.model_json_schema())\n",
      " |          # {'title': 'PromptInput', 'type': 'object', 'properties':\n",
      " |          {'question': {'title': 'Question', 'type': 'string'}}}\n",
      " |          print(chain_with_assign.output_schema.model_json_schema())\n",
      " |          # {'title': 'RunnableSequenceOutput', 'type': 'object', 'properties':\n",
      " |          {'str': {'title': 'Str',\n",
      " |          'type': 'string'}, 'hello': {'title': 'Hello', 'type': 'string'}}}\n",
      " |      \n",
      " |      Args:\n",
      " |          **kwargs: A mapping of keys to ``Runnable`` or ``Runnable``-like objects\n",
      " |              that will be invoked with the entire output dict of this ``Runnable``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new ``Runnable``.\n",
      " |  \n",
      " |  async astream(self, input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      " |      Default implementation of ``astream``, which calls ``ainvoke``.\n",
      " |      \n",
      " |      Subclasses should override this method if they support streaming output.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The input to the ``Runnable``.\n",
      " |          config: The config to use for the ``Runnable``. Defaults to None.\n",
      " |          kwargs: Additional keyword arguments to pass to the ``Runnable``.\n",
      " |      \n",
      " |      Yields:\n",
      " |          The output of the ``Runnable``.\n",
      " |  \n",
      " |  async astream_events(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, version: \"Literal['v1', 'v2']\" = 'v2', include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[StreamEvent]'\n",
      " |      Generate a stream of events.\n",
      " |      \n",
      " |      Use to create an iterator over ``StreamEvents`` that provide real-time information\n",
      " |      about the progress of the ``Runnable``, including ``StreamEvents`` from intermediate\n",
      " |      results.\n",
      " |      \n",
      " |      A ``StreamEvent`` is a dictionary with the following schema:\n",
      " |      \n",
      " |      - ``event``: **str** - Event names are of the format:\n",
      " |        ``on_[runnable_type]_(start|stream|end)``.\n",
      " |      - ``name``: **str** - The name of the ``Runnable`` that generated the event.\n",
      " |      - ``run_id``: **str** - randomly generated ID associated with the given\n",
      " |        execution of the ``Runnable`` that emitted the event. A child ``Runnable`` that gets\n",
      " |        invoked as part of the execution of a parent ``Runnable`` is assigned its own\n",
      " |        unique ID.\n",
      " |      - ``parent_ids``: **list[str]** - The IDs of the parent runnables that generated\n",
      " |        the event. The root ``Runnable`` will have an empty list. The order of the parent\n",
      " |        IDs is from the root to the immediate parent. Only available for v2 version of\n",
      " |        the API. The v1 version of the API will return an empty list.\n",
      " |      - ``tags``: **Optional[list[str]]** - The tags of the ``Runnable`` that generated\n",
      " |        the event.\n",
      " |      - ``metadata``: **Optional[dict[str, Any]]** - The metadata of the ``Runnable`` that\n",
      " |        generated the event.\n",
      " |      - ``data``: **dict[str, Any]**\n",
      " |      \n",
      " |      Below is a table that illustrates some events that might be emitted by various\n",
      " |      chains. Metadata fields have been omitted from the table for brevity.\n",
      " |      Chain definitions have been included after the table.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This reference table is for the v2 version of the schema.\n",
      " |      \n",
      " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
      " |      | event                    | name             | chunk                               | input                                             | output                                              |\n",
      " |      +==========================+==================+=====================================+===================================================+=====================================================+\n",
      " |      | ``on_chat_model_start``  | [model name]     |                                     | ``{\"messages\": [[SystemMessage, HumanMessage]]}`` |                                                     |\n",
      " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
      " |      | ``on_chat_model_stream`` | [model name]     | ``AIMessageChunk(content=\"hello\")`` |                                                   |                                                     |\n",
      " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
      " |      | ``on_chat_model_end``    | [model name]     |                                     | ``{\"messages\": [[SystemMessage, HumanMessage]]}`` | ``AIMessageChunk(content=\"hello world\")``           |\n",
      " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
      " |      | ``on_llm_start``         | [model name]     |                                     | ``{'input': 'hello'}``                            |                                                     |\n",
      " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
      " |      | ``on_llm_stream``        | [model name]     | ``'Hello' ``                        |                                                   |                                                     |\n",
      " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
      " |      | ``on_llm_end``           | [model name]     |                                     | ``'Hello human!'``                                |                                                     |\n",
      " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
      " |      | ``on_chain_start``       | format_docs      |                                     |                                                   |                                                     |\n",
      " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
      " |      | ``on_chain_stream``      | format_docs      | ``'hello world!, goodbye world!'``  |                                                   |                                                     |\n",
      " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
      " |      | ``on_chain_end``         | format_docs      |                                     | ``[Document(...)]``                               | ``'hello world!, goodbye world!'``                  |\n",
      " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
      " |      | ``on_tool_start``        | some_tool        |                                     | ``{\"x\": 1, \"y\": \"2\"}``                            |                                                     |\n",
      " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
      " |      | ``on_tool_end``          | some_tool        |                                     |                                                   | ``{\"x\": 1, \"y\": \"2\"}``                              |\n",
      " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
      " |      | ``on_retriever_start``   | [retriever name] |                                     | ``{\"query\": \"hello\"}``                            |                                                     |\n",
      " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
      " |      | ``on_retriever_end``     | [retriever name] |                                     | ``{\"query\": \"hello\"}``                            | ``[Document(...), ..]``                             |\n",
      " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
      " |      | ``on_prompt_start``      | [template_name]  |                                     | ``{\"question\": \"hello\"}``                         |                                                     |\n",
      " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
      " |      | ``on_prompt_end``        | [template_name]  |                                     | ``{\"question\": \"hello\"}``                         | ``ChatPromptValue(messages: [SystemMessage, ...])`` |\n",
      " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
      " |      \n",
      " |      In addition to the standard events, users can also dispatch custom events (see example below).\n",
      " |      \n",
      " |      Custom events will be only be surfaced with in the v2 version of the API!\n",
      " |      \n",
      " |      A custom event has following format:\n",
      " |      \n",
      " |      +-----------+------+-----------------------------------------------------------------------------------------------------------+\n",
      " |      | Attribute | Type | Description                                                                                               |\n",
      " |      +===========+======+===========================================================================================================+\n",
      " |      | name      | str  | A user defined name for the event.                                                                        |\n",
      " |      +-----------+------+-----------------------------------------------------------------------------------------------------------+\n",
      " |      | data      | Any  | The data associated with the event. This can be anything, though we suggest making it JSON serializable.  |\n",
      " |      +-----------+------+-----------------------------------------------------------------------------------------------------------+\n",
      " |      \n",
      " |      Here are declarations associated with the standard events shown above:\n",
      " |      \n",
      " |      ``format_docs``:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          def format_docs(docs: list[Document]) -> str:\n",
      " |              '''Format the docs.'''\n",
      " |              return \", \".join([doc.page_content for doc in docs])\n",
      " |      \n",
      " |      \n",
      " |          format_docs = RunnableLambda(format_docs)\n",
      " |      \n",
      " |      ``some_tool``:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          @tool\n",
      " |          def some_tool(x: int, y: str) -> dict:\n",
      " |              '''Some_tool.'''\n",
      " |              return {\"x\": x, \"y\": y}\n",
      " |      \n",
      " |      ``prompt``:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          template = ChatPromptTemplate.from_messages(\n",
      " |              [\n",
      " |                  (\"system\", \"You are Cat Agent 007\"),\n",
      " |                  (\"human\", \"{question}\"),\n",
      " |              ]\n",
      " |          ).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\n",
      " |      \n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |      \n",
      " |          async def reverse(s: str) -> str:\n",
      " |              return s[::-1]\n",
      " |      \n",
      " |      \n",
      " |          chain = RunnableLambda(func=reverse)\n",
      " |      \n",
      " |          events = [\n",
      " |              event async for event in chain.astream_events(\"hello\", version=\"v2\")\n",
      " |          ]\n",
      " |      \n",
      " |          # will produce the following events (run_id, and parent_ids\n",
      " |          # has been omitted for brevity):\n",
      " |          [\n",
      " |              {\n",
      " |                  \"data\": {\"input\": \"hello\"},\n",
      " |                  \"event\": \"on_chain_start\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"chunk\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_stream\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"output\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_end\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |          ]\n",
      " |      \n",
      " |      \n",
      " |      Example: Dispatch Custom Event\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_core.callbacks.manager import (\n",
      " |              adispatch_custom_event,\n",
      " |          )\n",
      " |          from langchain_core.runnables import RunnableLambda, RunnableConfig\n",
      " |          import asyncio\n",
      " |      \n",
      " |      \n",
      " |          async def slow_thing(some_input: str, config: RunnableConfig) -> str:\n",
      " |              \"\"\"Do something that takes a long time.\"\"\"\n",
      " |              await asyncio.sleep(1) # Placeholder for some slow operation\n",
      " |              await adispatch_custom_event(\n",
      " |                  \"progress_event\",\n",
      " |                  {\"message\": \"Finished step 1 of 3\"},\n",
      " |                  config=config # Must be included for python < 3.10\n",
      " |              )\n",
      " |              await asyncio.sleep(1) # Placeholder for some slow operation\n",
      " |              await adispatch_custom_event(\n",
      " |                  \"progress_event\",\n",
      " |                  {\"message\": \"Finished step 2 of 3\"},\n",
      " |                  config=config # Must be included for python < 3.10\n",
      " |              )\n",
      " |              await asyncio.sleep(1) # Placeholder for some slow operation\n",
      " |              return \"Done\"\n",
      " |      \n",
      " |          slow_thing = RunnableLambda(slow_thing)\n",
      " |      \n",
      " |          async for event in slow_thing.astream_events(\"some_input\", version=\"v2\"):\n",
      " |              print(event)\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The input to the ``Runnable``.\n",
      " |          config: The config to use for the ``Runnable``.\n",
      " |          version: The version of the schema to use either ``'v2'`` or ``'v1'``.\n",
      " |                   Users should use ``'v2'``.\n",
      " |                   ``'v1'`` is for backwards compatibility and will be deprecated\n",
      " |                   in 0.4.0.\n",
      " |                   No default will be assigned until the API is stabilized.\n",
      " |                   custom events will only be surfaced in ``'v2'``.\n",
      " |          include_names: Only include events from ``Runnables`` with matching names.\n",
      " |          include_types: Only include events from ``Runnables`` with matching types.\n",
      " |          include_tags: Only include events from ``Runnables`` with matching tags.\n",
      " |          exclude_names: Exclude events from ``Runnables`` with matching names.\n",
      " |          exclude_types: Exclude events from ``Runnables`` with matching types.\n",
      " |          exclude_tags: Exclude events from ``Runnables`` with matching tags.\n",
      " |          kwargs: Additional keyword arguments to pass to the ``Runnable``.\n",
      " |              These will be passed to ``astream_log`` as this implementation\n",
      " |              of ``astream_events`` is built on top of ``astream_log``.\n",
      " |      \n",
      " |      Yields:\n",
      " |          An async stream of ``StreamEvents``.\n",
      " |      \n",
      " |      Raises:\n",
      " |          NotImplementedError: If the version is not ``'v1'`` or ``'v2'``.\n",
      " |  \n",
      " |  async astream_log(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, diff: 'bool' = True, with_streamed_output_list: 'bool' = True, include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]]'\n",
      " |      Stream all output from a ``Runnable``, as reported to the callback system.\n",
      " |      \n",
      " |      This includes all inner runs of LLMs, Retrievers, Tools, etc.\n",
      " |      \n",
      " |      Output is streamed as Log objects, which include a list of\n",
      " |      Jsonpatch ops that describe how the state of the run has changed in each\n",
      " |      step, and the final state of the run.\n",
      " |      \n",
      " |      The Jsonpatch ops can be applied in order to construct state.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The input to the ``Runnable``.\n",
      " |          config: The config to use for the ``Runnable``.\n",
      " |          diff: Whether to yield diffs between each step or the current state.\n",
      " |          with_streamed_output_list: Whether to yield the ``streamed_output`` list.\n",
      " |          include_names: Only include logs with these names.\n",
      " |          include_types: Only include logs with these types.\n",
      " |          include_tags: Only include logs with these tags.\n",
      " |          exclude_names: Exclude logs with these names.\n",
      " |          exclude_types: Exclude logs with these types.\n",
      " |          exclude_tags: Exclude logs with these tags.\n",
      " |          kwargs: Additional keyword arguments to pass to the ``Runnable``.\n",
      " |      \n",
      " |      Yields:\n",
      " |          A ``RunLogPatch`` or ``RunLog`` object.\n",
      " |  \n",
      " |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      " |      Transform inputs to outputs.\n",
      " |      \n",
      " |      Default implementation of atransform, which buffers input and calls ``astream``.\n",
      " |      \n",
      " |      Subclasses should override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: An async iterator of inputs to the ``Runnable``.\n",
      " |          config: The config to use for the ``Runnable``. Defaults to None.\n",
      " |          kwargs: Additional keyword arguments to pass to the ``Runnable``.\n",
      " |      \n",
      " |      Yields:\n",
      " |          The output of the ``Runnable``.\n",
      " |  \n",
      " |  batch(self, inputs: 'list[Input]', config: 'Optional[Union[RunnableConfig, list[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'list[Output]'\n",
      " |      Default implementation runs invoke in parallel using a thread pool executor.\n",
      " |      \n",
      " |      The default implementation of batch works well for IO bound runnables.\n",
      " |      \n",
      " |      Subclasses should override this method if they can batch more efficiently;\n",
      " |      e.g., if the underlying ``Runnable`` uses an API which supports a batch mode.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: A list of inputs to the ``Runnable``.\n",
      " |          config: A config to use when invoking the ``Runnable``. The config supports\n",
      " |              standard keys like ``'tags'``, ``'metadata'`` for\n",
      " |              tracing purposes, ``'max_concurrency'`` for controlling how much work\n",
      " |              to do in parallel, and other keys. Please refer to the\n",
      " |              ``RunnableConfig`` for more details. Defaults to None.\n",
      " |          return_exceptions: Whether to return exceptions instead of raising them.\n",
      " |              Defaults to False.\n",
      " |          **kwargs: Additional keyword arguments to pass to the ``Runnable``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of outputs from the ``Runnable``.\n",
      " |  \n",
      " |  batch_as_completed(self, inputs: 'Sequence[Input]', config: 'Optional[Union[RunnableConfig, Sequence[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'Iterator[tuple[int, Union[Output, Exception]]]'\n",
      " |      Run ``invoke`` in parallel on a list of inputs.\n",
      " |      \n",
      " |      Yields results as they complete.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: A list of inputs to the ``Runnable``.\n",
      " |          config: A config to use when invoking the ``Runnable``.\n",
      " |              The config supports standard keys like ``'tags'``, ``'metadata'`` for\n",
      " |              tracing purposes, ``'max_concurrency'`` for controlling how much work to\n",
      " |              do in parallel, and other keys. Please refer to the ``RunnableConfig``\n",
      " |              for more details. Defaults to None.\n",
      " |          return_exceptions: Whether to return exceptions instead of raising them.\n",
      " |              Defaults to False.\n",
      " |          **kwargs: Additional keyword arguments to pass to the ``Runnable``.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Tuples of the index of the input and the output from the ``Runnable``.\n",
      " |  \n",
      " |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind arguments to a ``Runnable``, returning a new ``Runnable``.\n",
      " |      \n",
      " |      Useful when a ``Runnable`` in a chain requires an argument that is not\n",
      " |      in the output of the previous ``Runnable`` or included in the user input.\n",
      " |      \n",
      " |      Args:\n",
      " |          kwargs: The arguments to bind to the ``Runnable``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new ``Runnable`` with the arguments bound.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_ollama import ChatOllama\n",
      " |          from langchain_core.output_parsers import StrOutputParser\n",
      " |      \n",
      " |          llm = ChatOllama(model=\"llama2\")\n",
      " |      \n",
      " |          # Without bind.\n",
      " |          chain = llm | StrOutputParser()\n",
      " |      \n",
      " |          chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
      " |          # Output is 'One two three four five.'\n",
      " |      \n",
      " |          # With bind.\n",
      " |          chain = llm.bind(stop=[\"three\"]) | StrOutputParser()\n",
      " |      \n",
      " |          chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
      " |          # Output is 'One two'\n",
      " |  \n",
      " |  config_schema(self, *, include: 'Optional[Sequence[str]]' = None) -> 'type[BaseModel]'\n",
      " |      The type of config this ``Runnable`` accepts specified as a pydantic model.\n",
      " |      \n",
      " |      To mark a field as configurable, see the ``configurable_fields``\n",
      " |      and ``configurable_alternatives`` methods.\n",
      " |      \n",
      " |      Args:\n",
      " |          include: A list of fields to include in the config schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate config.\n",
      " |  \n",
      " |  get_config_jsonschema(self, *, include: 'Optional[Sequence[str]]' = None) -> 'dict[str, Any]'\n",
      " |      Get a JSON schema that represents the config of the ``Runnable``.\n",
      " |      \n",
      " |      Args:\n",
      " |          include: A list of fields to include in the config schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A JSON schema that represents the config of the ``Runnable``.\n",
      " |      \n",
      " |      .. versionadded:: 0.3.0\n",
      " |  \n",
      " |  get_graph(self, config: 'Optional[RunnableConfig]' = None) -> 'Graph'\n",
      " |      Return a graph representation of this ``Runnable``.\n",
      " |  \n",
      " |  get_input_jsonschema(self, config: 'Optional[RunnableConfig]' = None) -> 'dict[str, Any]'\n",
      " |      Get a JSON schema that represents the input to the ``Runnable``.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A JSON schema that represents the input to the ``Runnable``.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |      \n",
      " |              def add_one(x: int) -> int:\n",
      " |                  return x + 1\n",
      " |      \n",
      " |      \n",
      " |              runnable = RunnableLambda(add_one)\n",
      " |      \n",
      " |              print(runnable.get_input_jsonschema())\n",
      " |      \n",
      " |      .. versionadded:: 0.3.0\n",
      " |  \n",
      " |  get_name(self, suffix: 'Optional[str]' = None, *, name: 'Optional[str]' = None) -> 'str'\n",
      " |      Get the name of the ``Runnable``.\n",
      " |      \n",
      " |      Args:\n",
      " |          suffix: An optional suffix to append to the name.\n",
      " |          name: An optional name to use instead of the ``Runnable``'s name.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The name of the ``Runnable``.\n",
      " |  \n",
      " |  get_output_jsonschema(self, config: 'Optional[RunnableConfig]' = None) -> 'dict[str, Any]'\n",
      " |      Get a JSON schema that represents the output of the ``Runnable``.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A JSON schema that represents the output of the ``Runnable``.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |      \n",
      " |              def add_one(x: int) -> int:\n",
      " |                  return x + 1\n",
      " |      \n",
      " |      \n",
      " |              runnable = RunnableLambda(add_one)\n",
      " |      \n",
      " |              print(runnable.get_output_jsonschema())\n",
      " |      \n",
      " |      .. versionadded:: 0.3.0\n",
      " |  \n",
      " |  get_prompts(self, config: 'Optional[RunnableConfig]' = None) -> 'list[BasePromptTemplate]'\n",
      " |      Return a list of prompts used by this ``Runnable``.\n",
      " |  \n",
      " |  map(self) -> 'Runnable[list[Input], list[Output]]'\n",
      " |      Return a new ``Runnable`` that maps a list of inputs to a list of outputs.\n",
      " |      \n",
      " |      Calls ``invoke`` with each input.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new ``Runnable`` that maps a list of inputs to a list of outputs.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |                  from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |      \n",
      " |                  def _lambda(x: int) -> int:\n",
      " |                      return x + 1\n",
      " |      \n",
      " |      \n",
      " |                  runnable = RunnableLambda(_lambda)\n",
      " |                  print(runnable.map().invoke([1, 2, 3]))  # [2, 3, 4]\n",
      " |  \n",
      " |  pick(self, keys: 'Union[str, list[str]]') -> 'RunnableSerializable[Any, Any]'\n",
      " |      Pick keys from the output dict of this ``Runnable``.\n",
      " |      \n",
      " |      Pick single key:\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              import json\n",
      " |      \n",
      " |              from langchain_core.runnables import RunnableLambda, RunnableMap\n",
      " |      \n",
      " |              as_str = RunnableLambda(str)\n",
      " |              as_json = RunnableLambda(json.loads)\n",
      " |              chain = RunnableMap(str=as_str, json=as_json)\n",
      " |      \n",
      " |              chain.invoke(\"[1, 2, 3]\")\n",
      " |              # -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3]}\n",
      " |      \n",
      " |              json_only_chain = chain.pick(\"json\")\n",
      " |              json_only_chain.invoke(\"[1, 2, 3]\")\n",
      " |              # -> [1, 2, 3]\n",
      " |      \n",
      " |      Pick list of keys:\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              from typing import Any\n",
      " |      \n",
      " |              import json\n",
      " |      \n",
      " |              from langchain_core.runnables import RunnableLambda, RunnableMap\n",
      " |      \n",
      " |              as_str = RunnableLambda(str)\n",
      " |              as_json = RunnableLambda(json.loads)\n",
      " |      \n",
      " |      \n",
      " |              def as_bytes(x: Any) -> bytes:\n",
      " |                  return bytes(x, \"utf-8\")\n",
      " |      \n",
      " |      \n",
      " |              chain = RunnableMap(\n",
      " |                  str=as_str, json=as_json, bytes=RunnableLambda(as_bytes)\n",
      " |              )\n",
      " |      \n",
      " |              chain.invoke(\"[1, 2, 3]\")\n",
      " |              # -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\n",
      " |      \n",
      " |              json_and_bytes_chain = chain.pick([\"json\", \"bytes\"])\n",
      " |              json_and_bytes_chain.invoke(\"[1, 2, 3]\")\n",
      " |              # -> {\"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\n",
      " |      \n",
      " |      Args:\n",
      " |          keys: A key or list of keys to pick from the output dict.\n",
      " |      \n",
      " |      Returns:\n",
      " |          a new ``Runnable``.\n",
      " |  \n",
      " |  pipe(self, *others: 'Union[Runnable[Any, Other], Callable[[Any], Other]]', name: 'Optional[str]' = None) -> 'RunnableSerializable[Input, Other]'\n",
      " |      Pipe runnables.\n",
      " |      \n",
      " |      Compose this ``Runnable`` with ``Runnable``-like objects to make a\n",
      " |      ``RunnableSequence``.\n",
      " |      \n",
      " |      Equivalent to ``RunnableSequence(self, *others)`` or ``self | others[0] | ...``\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |      \n",
      " |              def add_one(x: int) -> int:\n",
      " |                  return x + 1\n",
      " |      \n",
      " |      \n",
      " |              def mul_two(x: int) -> int:\n",
      " |                  return x * 2\n",
      " |      \n",
      " |      \n",
      " |              runnable_1 = RunnableLambda(add_one)\n",
      " |              runnable_2 = RunnableLambda(mul_two)\n",
      " |              sequence = runnable_1.pipe(runnable_2)\n",
      " |              # Or equivalently:\n",
      " |              # sequence = runnable_1 | runnable_2\n",
      " |              # sequence = RunnableSequence(first=runnable_1, last=runnable_2)\n",
      " |              sequence.invoke(1)\n",
      " |              await sequence.ainvoke(1)\n",
      " |              # -> 4\n",
      " |      \n",
      " |              sequence.batch([1, 2, 3])\n",
      " |              await sequence.abatch([1, 2, 3])\n",
      " |              # -> [4, 6, 8]\n",
      " |      \n",
      " |      Args:\n",
      " |          *others: Other ``Runnable`` or ``Runnable``-like objects to compose\n",
      " |          name: An optional name for the resulting ``RunnableSequence``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new ``Runnable``.\n",
      " |  \n",
      " |  stream(self, input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      " |      Default implementation of ``stream``, which calls ``invoke``.\n",
      " |      \n",
      " |      Subclasses should override this method if they support streaming output.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The input to the ``Runnable``.\n",
      " |          config: The config to use for the ``Runnable``. Defaults to None.\n",
      " |          kwargs: Additional keyword arguments to pass to the ``Runnable``.\n",
      " |      \n",
      " |      Yields:\n",
      " |          The output of the ``Runnable``.\n",
      " |  \n",
      " |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      " |      Transform inputs to outputs.\n",
      " |      \n",
      " |      Default implementation of transform, which buffers input and calls ``astream``.\n",
      " |      \n",
      " |      Subclasses should override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: An iterator of inputs to the ``Runnable``.\n",
      " |          config: The config to use for the ``Runnable``. Defaults to None.\n",
      " |          kwargs: Additional keyword arguments to pass to the ``Runnable``.\n",
      " |      \n",
      " |      Yields:\n",
      " |          The output of the ``Runnable``.\n",
      " |  \n",
      " |  with_alisteners(self, *, on_start: 'Optional[AsyncListener]' = None, on_end: 'Optional[AsyncListener]' = None, on_error: 'Optional[AsyncListener]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind async lifecycle listeners to a ``Runnable``.\n",
      " |      \n",
      " |      Returns a new ``Runnable``.\n",
      " |      \n",
      " |      The Run object contains information about the run, including its ``id``,\n",
      " |      ``type``, ``input``, ``output``, ``error``, ``start_time``, ``end_time``, and\n",
      " |      any tags or metadata added to the run.\n",
      " |      \n",
      " |      Args:\n",
      " |          on_start: Called asynchronously before the ``Runnable`` starts running,\n",
      " |              with the ``Run`` object. Defaults to None.\n",
      " |          on_end: Called asynchronously after the ``Runnable`` finishes running,\n",
      " |              with the ``Run`` object. Defaults to None.\n",
      " |          on_error: Called asynchronously if the ``Runnable`` throws an error,\n",
      " |              with the ``Run`` object. Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new ``Runnable`` with the listeners bound.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_core.runnables import RunnableLambda, Runnable\n",
      " |          from datetime import datetime, timezone\n",
      " |          import time\n",
      " |          import asyncio\n",
      " |      \n",
      " |          def format_t(timestamp: float) -> str:\n",
      " |              return datetime.fromtimestamp(timestamp, tz=timezone.utc).isoformat()\n",
      " |      \n",
      " |          async def test_runnable(time_to_sleep : int):\n",
      " |              print(f\"Runnable[{time_to_sleep}s]: starts at {format_t(time.time())}\")\n",
      " |              await asyncio.sleep(time_to_sleep)\n",
      " |              print(f\"Runnable[{time_to_sleep}s]: ends at {format_t(time.time())}\")\n",
      " |      \n",
      " |          async def fn_start(run_obj : Runnable):\n",
      " |              print(f\"on start callback starts at {format_t(time.time())}\")\n",
      " |              await asyncio.sleep(3)\n",
      " |              print(f\"on start callback ends at {format_t(time.time())}\")\n",
      " |      \n",
      " |          async def fn_end(run_obj : Runnable):\n",
      " |              print(f\"on end callback starts at {format_t(time.time())}\")\n",
      " |              await asyncio.sleep(2)\n",
      " |              print(f\"on end callback ends at {format_t(time.time())}\")\n",
      " |      \n",
      " |          runnable = RunnableLambda(test_runnable).with_alisteners(\n",
      " |              on_start=fn_start,\n",
      " |              on_end=fn_end\n",
      " |          )\n",
      " |          async def concurrent_runs():\n",
      " |              await asyncio.gather(runnable.ainvoke(2), runnable.ainvoke(3))\n",
      " |      \n",
      " |          asyncio.run(concurrent_runs())\n",
      " |          Result:\n",
      " |          on start callback starts at 2025-03-01T07:05:22.875378+00:00\n",
      " |          on start callback starts at 2025-03-01T07:05:22.875495+00:00\n",
      " |          on start callback ends at 2025-03-01T07:05:25.878862+00:00\n",
      " |          on start callback ends at 2025-03-01T07:05:25.878947+00:00\n",
      " |          Runnable[2s]: starts at 2025-03-01T07:05:25.879392+00:00\n",
      " |          Runnable[3s]: starts at 2025-03-01T07:05:25.879804+00:00\n",
      " |          Runnable[2s]: ends at 2025-03-01T07:05:27.881998+00:00\n",
      " |          on end callback starts at 2025-03-01T07:05:27.882360+00:00\n",
      " |          Runnable[3s]: ends at 2025-03-01T07:05:28.881737+00:00\n",
      " |          on end callback starts at 2025-03-01T07:05:28.882428+00:00\n",
      " |          on end callback ends at 2025-03-01T07:05:29.883893+00:00\n",
      " |          on end callback ends at 2025-03-01T07:05:30.884831+00:00\n",
      " |  \n",
      " |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind config to a ``Runnable``, returning a new ``Runnable``.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: The config to bind to the ``Runnable``.\n",
      " |          kwargs: Additional keyword arguments to pass to the ``Runnable``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new ``Runnable`` with the config bound.\n",
      " |  \n",
      " |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'tuple[type[BaseException], ...]' = (<class 'Exception'>,), exception_key: 'Optional[str]' = None) -> 'RunnableWithFallbacksT[Input, Output]'\n",
      " |      Add fallbacks to a ``Runnable``, returning a new ``Runnable``.\n",
      " |      \n",
      " |      The new ``Runnable`` will try the original ``Runnable``, and then each fallback\n",
      " |      in order, upon failures.\n",
      " |      \n",
      " |      Args:\n",
      " |          fallbacks: A sequence of runnables to try if the original ``Runnable``\n",
      " |              fails.\n",
      " |          exceptions_to_handle: A tuple of exception types to handle.\n",
      " |              Defaults to ``(Exception,)``.\n",
      " |          exception_key: If string is specified then handled exceptions will be passed\n",
      " |              to fallbacks as part of the input under the specified key.\n",
      " |              If None, exceptions will not be passed to fallbacks.\n",
      " |              If used, the base ``Runnable`` and its fallbacks must accept a\n",
      " |              dictionary as input. Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new ``Runnable`` that will try the original ``Runnable``, and then each\n",
      " |          fallback in order, upon failures.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              from typing import Iterator\n",
      " |      \n",
      " |              from langchain_core.runnables import RunnableGenerator\n",
      " |      \n",
      " |      \n",
      " |              def _generate_immediate_error(input: Iterator) -> Iterator[str]:\n",
      " |                  raise ValueError()\n",
      " |                  yield \"\"\n",
      " |      \n",
      " |      \n",
      " |              def _generate(input: Iterator) -> Iterator[str]:\n",
      " |                  yield from \"foo bar\"\n",
      " |      \n",
      " |      \n",
      " |              runnable = RunnableGenerator(_generate_immediate_error).with_fallbacks(\n",
      " |                  [RunnableGenerator(_generate)]\n",
      " |              )\n",
      " |              print(\"\".join(runnable.stream({})))  # foo bar\n",
      " |      \n",
      " |      Args:\n",
      " |          fallbacks: A sequence of runnables to try if the original ``Runnable``\n",
      " |              fails.\n",
      " |          exceptions_to_handle: A tuple of exception types to handle.\n",
      " |          exception_key: If string is specified then handled exceptions will be passed\n",
      " |              to fallbacks as part of the input under the specified key.\n",
      " |              If None, exceptions will not be passed to fallbacks.\n",
      " |              If used, the base ``Runnable`` and its fallbacks must accept a\n",
      " |              dictionary as input.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new ``Runnable`` that will try the original ``Runnable``, and then each\n",
      " |          fallback in order, upon failures.\n",
      " |  \n",
      " |  with_listeners(self, *, on_start: 'Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]' = None, on_end: 'Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]' = None, on_error: 'Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind lifecycle listeners to a ``Runnable``, returning a new ``Runnable``.\n",
      " |      \n",
      " |      The Run object contains information about the run, including its ``id``,\n",
      " |      ``type``, ``input``, ``output``, ``error``, ``start_time``, ``end_time``, and\n",
      " |      any tags or metadata added to the run.\n",
      " |      \n",
      " |      Args:\n",
      " |          on_start: Called before the ``Runnable`` starts running, with the ``Run``\n",
      " |              object. Defaults to None.\n",
      " |          on_end: Called after the ``Runnable`` finishes running, with the ``Run``\n",
      " |              object. Defaults to None.\n",
      " |          on_error: Called if the ``Runnable`` throws an error, with the ``Run``\n",
      " |              object. Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new ``Runnable`` with the listeners bound.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |          from langchain_core.tracers.schemas import Run\n",
      " |      \n",
      " |          import time\n",
      " |      \n",
      " |      \n",
      " |          def test_runnable(time_to_sleep: int):\n",
      " |              time.sleep(time_to_sleep)\n",
      " |      \n",
      " |      \n",
      " |          def fn_start(run_obj: Run):\n",
      " |              print(\"start_time:\", run_obj.start_time)\n",
      " |      \n",
      " |      \n",
      " |          def fn_end(run_obj: Run):\n",
      " |              print(\"end_time:\", run_obj.end_time)\n",
      " |      \n",
      " |      \n",
      " |          chain = RunnableLambda(test_runnable).with_listeners(\n",
      " |              on_start=fn_start, on_end=fn_end\n",
      " |          )\n",
      " |          chain.invoke(2)\n",
      " |  \n",
      " |  with_retry(self, *, retry_if_exception_type: 'tuple[type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, exponential_jitter_params: 'Optional[ExponentialJitterParams]' = None, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      " |      Create a new Runnable that retries the original Runnable on exceptions.\n",
      " |      \n",
      " |      Args:\n",
      " |          retry_if_exception_type: A tuple of exception types to retry on.\n",
      " |              Defaults to (Exception,).\n",
      " |          wait_exponential_jitter: Whether to add jitter to the wait\n",
      " |              time between retries. Defaults to True.\n",
      " |          stop_after_attempt: The maximum number of attempts to make before\n",
      " |              giving up. Defaults to 3.\n",
      " |          exponential_jitter_params: Parameters for\n",
      " |              ``tenacity.wait_exponential_jitter``. Namely: ``initial``, ``max``,\n",
      " |              ``exp_base``, and ``jitter`` (all float values).\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable that retries the original Runnable on exceptions.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |          count = 0\n",
      " |      \n",
      " |      \n",
      " |          def _lambda(x: int) -> None:\n",
      " |              global count\n",
      " |              count = count + 1\n",
      " |              if x == 1:\n",
      " |                  raise ValueError(\"x is 1\")\n",
      " |              else:\n",
      " |                  pass\n",
      " |      \n",
      " |      \n",
      " |          runnable = RunnableLambda(_lambda)\n",
      " |          try:\n",
      " |              runnable.with_retry(\n",
      " |                  stop_after_attempt=2,\n",
      " |                  retry_if_exception_type=(ValueError,),\n",
      " |              ).invoke(1)\n",
      " |          except ValueError:\n",
      " |              pass\n",
      " |      \n",
      " |          assert count == 2\n",
      " |  \n",
      " |  with_types(self, *, input_type: 'Optional[type[Input]]' = None, output_type: 'Optional[type[Output]]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind input and output types to a ``Runnable``, returning a new ``Runnable``.\n",
      " |      \n",
      " |      Args:\n",
      " |          input_type: The input type to bind to the ``Runnable``. Defaults to None.\n",
      " |          output_type: The output type to bind to the ``Runnable``. Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable with the types bound.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.runnables.base.Runnable:\n",
      " |  \n",
      " |  InputType\n",
      " |      Input type.\n",
      " |      \n",
      " |      The type of input this ``Runnable`` accepts specified as a type annotation.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError: If the input type cannot be inferred.\n",
      " |  \n",
      " |  OutputType\n",
      " |      Output Type.\n",
      " |      \n",
      " |      The type of output this ``Runnable`` produces specified as a type annotation.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError: If the output type cannot be inferred.\n",
      " |  \n",
      " |  config_specs\n",
      " |      List configurable fields for this ``Runnable``.\n",
      " |  \n",
      " |  input_schema\n",
      " |      The type of input this ``Runnable`` accepts specified as a pydantic model.\n",
      " |  \n",
      " |  output_schema\n",
      " |      Output schema.\n",
      " |      \n",
      " |      The type of output this ``Runnable`` produces specified as a pydantic model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from pydantic._internal._model_construction.ModelMetaclass\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "04d827f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI: Hey there! ðŸ‘‹ Iâ€™m your friendly neighborhood AI, ready to chat, answer questions, or just keep you company. Iâ€™m powered by the latest GPTâ€‘4 architecture, and Iâ€™ve been trained on a massive mix of books, articles, websites, and even some fun triviaâ€”so feel free to throw anything my way. Whether youâ€™re curious about the science behind photosynthesis, need a recipe tweak, or just want to hear a joke, Iâ€™ve got you covered. \n",
      "\n",
      "By the way, itâ€™s currently November 28, 2025, so if youâ€™re planning a holiday or just wondering whatâ€™s trending this week, I can help with that too. Whatâ€™s on your mind today?\n",
      "Human: what is the most iconic place in Rome?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The single place that practically pops up on every â€œmustâ€‘seeâ€ list, in every travel brochure, and in the first thing people think of when they picture Rome is the **Colosseum** (officially the Flavian Amphitheatre). Itâ€™s the living, breathing heart of ancient Roman engineering and spectacle, and itâ€™s hard to argue with its iconic status.\\n\\n---\\n\\n## Why the Colosseum is Romeâ€™s poster child\\n\\n| Feature | What makes it iconic |\\n|---------|----------------------|\\n| **Size & Scale** | 6,000â€‘7,000 seats, 48â€‘meterâ€‘high, 16â€‘meterâ€‘wide arena. It dwarfs most modern stadiums. |\\n| **Engineering Marvel** | 80,000 tons of travertine, tuff, and concrete; a complex system of vaults, arches, and a retractable awning (velarium) that could shade the entire arena. |\\n| **Historical Significance** | Built 70â€‘80\\u202fAD under Vespasian and Titus, it was the site of gladiatorial contests, mock naval battles, animal hunts, and public executionsâ€”events that defined Roman public life. |\\n| **Cultural Resonance** | The image of the Colosseum is synonymous with Rome in films, postcards, and even the logo of the Italian national football team. |\\n| **Survival Story** | Withstood earthquakes, fires, and stoneâ€‘thieves for centuries, yet still stands largely intact. |\\n\\n---\\n\\n## A quick tour of what youâ€™ll see\\n\\n1. **The Outer Perimeter (Piazza del Colosseo)** â€“ The massive, open square thatâ€™s a hub for locals and tourists alike. Youâ€™ll find the **Arco della Pace** (Arch of Peace) and the **Museo del Colosseo** (Colosseum Museum) right on the edge.\\n\\n2. **The Inner Arena** â€“ Walk down the **hypogeum** (the underground network of tunnels and chambers) to see where gladiators and animals were staged. The museumâ€™s 3â€‘D reconstruction gives you a sense of the scale and the logistics of ancient spectacles.\\n\\n3. **The Upper Tier** â€“ The **tesserae** (stone tiles) that once covered the seating area are still visible. Theyâ€™re a reminder of the social hierarchy: emperors, senators, and the general public all had distinct sections.\\n\\n4. **The Velarium** â€“ While the original retractable awning is long gone, you can see the wooden framework that supported it. Imagine the roar of the crowd and the shade of that massive canvas!\\n\\n---\\n\\n## Fun facts to sprinkle into conversation\\n\\n- **Name Origin**: The name â€œColosseumâ€ comes from the nearby **Colossus of Nero**, a gigantic bronze statue that once stood in front of the amphitheater. The statue was later moved to the **Ara Pacis** (Altar of Peace) in the 1st century\\u202fAD.\\n- **Gladiator Training**: The **ludus** (gladiator school) was located just outside the Colosseum. Some of the training pits are still visible in the hypogeum.\\n- **Modern Use**: In the 20th century, the Colosseum was used as a filming location for movies like *Gladiator* (2000) and *The Last Samurai* (2003). It also hosted a 2009 concert by the Italian rock band **Eros Ramazzotti**.\\n- **Conservation Efforts**: The **Colosseum Restoration Project** (ongoing since the 1990s) has used laser scanning and 3â€‘D modeling to preserve the structure and plan future repairs.\\n\\n---\\n\\n## Visiting Tips\\n\\n| Tip | Why it matters |\\n|-----|----------------|\\n| **Buy tickets online** | Skip the long lines (especially in peak season). |\\n| **Visit early or late** | The light is softer, and the crowds are thinner. |\\n| **Take a guided tour** | A knowledgeable guide can point out hidden details (like the **tomb of the Emperor Titus** in the hypogeum). |\\n| **Combine with nearby sites** | The Roman Forum and Palatine Hill are right next doorâ€”plan a halfâ€‘day itinerary. |\\n| **Wear comfortable shoes** | The stone steps and uneven surfaces can be tough on feet. |\\n\\n---\\n\\n## Other iconic spots you might want to add to your list\\n\\n- **St.\\u202fPeterâ€™s Basilica** (Vatican City) â€“ The largest church in the world, with Michelangeloâ€™s dome and Berniniâ€™s Baldachin.\\n- **Trevi Fountain** â€“ Toss a coin and make a wish; the baroque masterpiece is a mustâ€‘see at night.\\n- **Pantheon** â€“ A marvel of ancient Roman architecture with its massive dome and oculus.\\n- **Spanish Steps** â€“ A social hub where locals and tourists mingle.\\n- **Piazza Navona** â€“ A baroque square with Berniniâ€™s Fountain of the Four Rivers.\\n\\n---\\n\\n### Bottom line\\n\\nIf youâ€™re looking for the single, unmistakable symbol of Rome that captures its ancient grandeur, its architectural genius, and its enduring cultural impact, the **Colosseum** is the place to go. Whether youâ€™re a history buff, an architecture enthusiast, or just someone who loves a good postcard, the Colosseum offers a tangible, aweâ€‘inspiring connection to the pastâ€”and a spectacular backdrop for your next Instagram photo. Enjoy your Roman adventure! ðŸ‡®ðŸ‡¹âœ¨'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.run(\"what is the most iconic place in Rome?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96e90ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI: Hey there! ðŸ‘‹ Iâ€™m thrilled you dropped by. Howâ€™s your day shaping up? Anything exciting on the horizonâ€”maybe a new book youâ€™re into, a recipe youâ€™re trying to perfect, or a project thatâ€™s got you buzzing? Iâ€™m all ears (and a bit of a chatterbox), so feel free to share whateverâ€™s on your mind!\n",
      "Human: What kind of other events?\n",
      "AI: Hey, great question! ðŸŽ‰ â€œOther eventsâ€ can mean a whole spectrum of things depending on the contextâ€”so letâ€™s explore a few common categories and sprinkle in some specifics to give you a taste of whatâ€™s out there. If you had a particular setting in mind (like a workplace, a community, or a hobby group), just let me know and I can zoom in even more!\n",
      "\n",
      "---\n",
      "\n",
      "## 1. **Social & Community Events**\n",
      "| Type | Typical Features | Why People Love Them |\n",
      "|------|------------------|----------------------|\n",
      "| **Meetâ€‘ups & Networking** | Casual gatherings, often centered around a shared interest (e.g., book clubs, hiking groups, tech meetâ€‘ups). | Great for making new friends, swapping stories, and discovering hidden gems in your city. |\n",
      "| **Festivals & Fairs** | Food stalls, live music, art installations, carnival rides. | Immersive, sensory experiences that celebrate culture, food, or seasonal themes. |\n",
      "| **Volunteer Drives** | Community cleanâ€‘ups, charity runs, food bank collections. | Combines socializing with a feelâ€‘good missionâ€”plus you get to see tangible impact. |\n",
      "| **Game Nights & Trivia** | Board games, video game tournaments, pub quizzes. | Friendly competition, laughs, and a chance to showcase your trivia prowess. |\n",
      "\n",
      "---\n",
      "\n",
      "## 2. **Professional & Industry Events**\n",
      "| Type | Typical Features | Why Professionals Attend |\n",
      "|------|------------------|--------------------------|\n",
      "| **Conferences & Summits** | Keynote speakers, breakout sessions, expo halls. | Deep dives into industry trends, networking with thought leaders, and often certification opportunities. |\n",
      "| **Workshops & Masterclasses** | Handsâ€‘on training, skillâ€‘building sessions. | Practical, immediate takeâ€‘awaysâ€”perfect for upskilling or learning a new tool. |\n",
      "| **Product Launches & Demo Days** | Live product demos, beta testing, investor pitches. | Firstâ€‘hand look at cuttingâ€‘edge tech and the chance to influence product direction. |\n",
      "| **Career Fairs & Recruitment Drives** | Company booths, resume dropâ€‘offs, onâ€‘site interviews. | Direct access to hiring managers and a chance to land a job on the spot. |\n",
      "\n",
      "---\n",
      "\n",
      "## 3. **Educational & Academic Events**\n",
      "| Type | Typical Features | Why Students & Educators Attend |\n",
      "|------|------------------|---------------------------------|\n",
      "| **Lectures & Guest Talks** | Subjectâ€‘specific talks by experts, Q&A sessions. | Exposure to new ideas, networking with scholars, and often free or lowâ€‘cost. |\n",
      "| **Research Symposia** | Paper presentations, poster sessions, panel discussions. | Showcase research, receive feedback, and collaborate across disciplines. |\n",
      "| **Study Groups & Review Sessions** | Group problemâ€‘solving, exam prep workshops. | Peer learning, accountability, and a supportive environment. |\n",
      "| **Academic Conferences** | Multiâ€‘day events with sessions, social mixers. | Build a CV, publish findings, and meet peers worldwide. |\n",
      "\n",
      "---\n",
      "\n",
      "## 4. **Cultural & Artistic Events**\n",
      "| Type | Typical Features | Why Art Lovers Attend |\n",
      "|------|------------------|-----------------------|\n",
      "| **Gallery Openings & Exhibitions** | Artist talks, live painting, interactive installations. | First look at new works, meet the creators, and support local art scenes. |\n",
      "| **Theater & Performance** | Plays, improv, dance recitals. | Live storytelling, emotional resonance, and often a chance to meet the cast. |\n",
      "| **Film Screenings & Festivals** | Indie films, retrospectives, Q&A with directors. | Discover fresh voices, enjoy cinematic artistry, and engage in postâ€‘film discussions. |\n",
      "| **Music Concerts & Recitals** | Classical, jazz, indie, or pop performances. | Live sound, communal energy, and the thrill of seeing musicians in action. |\n",
      "\n",
      "---\n",
      "\n",
      "## 5. **Health & Wellness Events**\n",
      "| Type | Typical Features | Why People Attend |\n",
      "|------|------------------|-------------------|\n",
      "| **Yoga & Meditation Retreats** | Guided sessions, nature settings, mindfulness workshops. | Reset, recharge, and connect with likeâ€‘minded souls. |\n",
      "| **Fitness Challenges** | 30â€‘day runs, cycling marathons, obstacle courses. | Push limits, celebrate milestones, and build camaraderie. |\n",
      "| **Health Fairs** | Free screenings, nutrition talks, wellness product demos. | Learn about preventive care and discover new health hacks. |\n",
      "| **Mental Health Workshops** | Coping strategies, therapy groups, selfâ€‘care seminars. | Safe spaces to share, learn, and grow emotionally. |\n",
      "\n",
      "---\n",
      "\n",
      "## 6. **Special Interest & Hobby Events**\n",
      "| Type | Typical Features | Why Enthusiasts Attend |\n",
      "|------|------------------|------------------------|\n",
      "| **Maker Faires & Hackathons** | DIY projects, coding sprints, prototype showcases. | Handsâ€‘on creativity, collaboration, and often prizes or funding. |\n",
      "| **Cosplay Conventions** | Costume contests, panels, photo ops. | Express fandom, meet fellow cosplayers, and celebrate pop culture. |\n",
      "| **Cooking & Baking Classes** | Live demos, recipe swaps, tasting sessions. | Master new techniques, taste fresh dishes, and bond over food. |\n",
      "| **Photography Walks** | Guided tours, critique sessions, gear demos. | Capture the world, learn composition, and network with peers. |\n",
      "\n",
      "---\n",
      "\n",
      "### Quick Tips for Finding the Right Event\n",
      "1. **Check Local Listings** â€“ City event calendars, community boards, and neighborhood Facebook groups are goldmines.  \n",
      "2. **Use Event Platforms** â€“ Sites like Eventbrite, Meetup, and Facebook Events let you filter by interest, location, and date.  \n",
      "3. **Ask Around** â€“ Friends, coworkers, or online forums (Reddit, Discord) can recommend hidden gems.  \n",
      "4. **Set a Budget** â€“ Some events are free, others require tickets or membership fees; plan accordingly.  \n",
      "5. **Plan Ahead** â€“ Popular events sell out fastâ€”book early or RSVP to secure a spot.\n",
      "\n",
      "---\n",
      "\n",
      "## Want to Dive Deeper?\n",
      "If youâ€™re thinking about a specific type of eventâ€”say, a tech conference in San Francisco or a local farmersâ€™ marketâ€”just let me know! I can pull up the latest dates, highlight mustâ€‘see sessions, or even suggest nearby venues that match your vibe. ðŸŒŸ\n",
      "\n",
      "Hope this gives you a colorful map of the event landscape. Let me know which category sparks your curiosity, and weâ€™ll explore it together!\n",
      "Human: What kind of other events?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Absolutelyâ€”â€œother eventsâ€ is a huge umbrella, and the possibilities are almost endless once you start digging into niche interests, local traditions, or industryâ€‘specific happenings. Below is a deeper dive into a handful of categories that often slip under the radar but can be just as exciting (or even more so) than the mainstream festivals and conferences you might already know. Iâ€™ll sprinkle in some concrete examples, typical features, and why people flock to each type. If any of these spark your curiosity, let me know and we can zoom in on the exact dates, venues, or insider tips!\\n\\n---\\n\\n## 1. **Sports & Outdoor Adventure Events**\\n\\n| Type | Typical Features | Why Fans Attend |\\n|------|------------------|-----------------|\\n| **Ultraâ€‘Marathons & Trail Races** | 50â€‘km+ courses, rugged terrain, scenic vistas, community support stations. | Test endurance, enjoy nature, and celebrate finishing a grueling challenge. |\\n| **Adventure Races (e.g., Spartan, Tough Mudder)** | Obstacle courses, mud pits, teamâ€‘based challenges. | Fun, camaraderie, and a chance to push limits in a playful setting. |\\n| **Kayak & Paddleâ€‘board Festivals** | Guided tours, skill clinics, vendor booths, live music. | Combine water sports with community vibes and often include food trucks. |\\n| **Local Sports Tournaments (e.g., 5â€‘aâ€‘side soccer, ultimate frisbee)** | Shortâ€‘format games, openâ€‘toâ€‘all skill levels, social mixers afterward. | Casual competition, networking, and a lowâ€‘barrier entry point for newcomers. |\\n\\n---\\n\\n## 2. **Religious & Spiritual Gatherings**\\n\\n| Type | Typical Features | Why Participants Join |\\n|------|------------------|-----------------------|\\n| **Pilgrimages (e.g., Camino de Santiago, Hajj)** | Multiâ€‘day walking or travel, guided by faith leaders, communal meals. | Spiritual growth, cultural immersion, and a sense of shared purpose. |\\n| **Interfaith Dialogues & Peace Conferences** | Panel discussions, workshops, cultural performances. | Foster understanding, build bridges, and explore common values. |\\n| **Retreats & Silent Meditations** | Quiet spaces, guided breathing, minimal tech. | Deepen practice, reset mental health, and connect with likeâ€‘minded seekers. |\\n| **Festivals of Light (e.g., Diwali, Hanukkah, Christmas markets)** | Lanterns, music, traditional foods, craft stalls. | Celebrate heritage, share joy, and create lasting memories. |\\n\\n---\\n\\n## 3. **Political & Civic Engagement Events**\\n\\n| Type | Typical Features | Why Citizens Attend |\\n|------|------------------|---------------------|\\n| **Town Hall Meetings** | Open Q&A with local officials, policy briefings, community feedback. | Voice concerns, stay informed, and influence local decisions. |\\n| **Policy Hackathons** | Teams brainstorm solutions to civic challenges, pitch to city leaders. | Innovate, network with policymakers, and potentially see ideas implemented. |\\n| **Election Debates & Candidate Forums** | Live debates, moderated questions, audience participation. | Compare platforms, ask tough questions, and feel part of the democratic process. |\\n| **Community Cleanâ€‘Up Drives** | Volunteer signâ€‘ups, trash collection, recycling stations. | Beautify neighborhoods, build community pride, and get a tangible sense of impact. |\\n\\n---\\n\\n## 4. **Science & Tech Popâ€‘Up Events**\\n\\n| Type | Typical Features | Why Enthusiasts Attend |\\n|------|------------------|------------------------|\\n| **Science Fairs & Maker Fairs** | DIY projects, robotics demos, interactive exhibits. | Handsâ€‘on learning, inspiration for future projects, and networking with creators. |\\n| **Tech â€œUnconferenceâ€ Sessions** | Participantâ€‘led talks, lightning demos, collaborative problemâ€‘solving. | Flexibility, peerâ€‘toâ€‘peer learning, and a chance to shape the agenda. |\\n| **Space & Astronomy Nights** | Telescope viewings, planetarium shows, guest astronomers. | Stargazing, learning about the cosmos, and connecting with fellow stargazers. |\\n| **VR/AR Expo** | Immersive demos, game showcases, developer talks. | Experience cuttingâ€‘edge tech, meet creators, and test new hardware. |\\n\\n---\\n\\n## 5. **Culinary & Foodâ€‘Related Events**\\n\\n| Type | Typical Features | Why Foodies Attend |\\n|------|------------------|--------------------|\\n| **Popâ€‘Up Restaurants & Food Trucks** | Limitedâ€‘time menus, chef collaborations, unique locations. | Try exclusive dishes, support local chefs, and enjoy a novel dining experience. |\\n| **Farmersâ€™ Markets & CSA Events** | Fresh produce, artisanal goods, farmâ€‘toâ€‘table demos. | Directly support farmers, learn about seasonal ingredients, and taste local flavors. |\\n| **Wine & Beer Tastings (including microâ€‘breweries)** | Guided tastings, pairing workshops, meetâ€‘theâ€‘brewer sessions. | Discover new varieties, learn tasting techniques, and enjoy a relaxed social setting. |\\n| **Cooking Competitions (e.g., â€œChoppedâ€ style)** | Live cooking challenges, celebrity judges, audience voting. | Witness culinary creativity, cheer on contestants, and maybe win a prize. |\\n\\n---\\n\\n## 6. **Arts & Performance â€œMicroâ€‘Eventsâ€**\\n\\n| Type | Typical Features | Why Attendees Love Them |\\n|------|------------------|------------------------|\\n| **Openâ€‘Mic Poetry & Spoken Word** | Local artists perform, audience interaction, sometimes themed nights. | Raw, intimate performances, community support, and a chance to share your own work. |\\n| **Flash Mobs & Street Performances** | Surprise performances in public spaces, often with a social message. | Unexpected joy, collective participation, and a memorable urban experience. |\\n| **Microâ€‘Theater (e.g., 15â€‘minute plays)** | Short, often experimental productions, sometimes in unconventional venues. | Quick, intense storytelling, and a chance to see avantâ€‘garde work. |\\n| **DIY Art Installations (e.g., community murals)** | Collaborative creation, public art projects, often with a social cause. | Express creativity, leave a lasting mark, and engage with neighbors. |\\n\\n---\\n\\n## 7. **Special Interest & Hobbyist Gatherings**\\n\\n| Type | Typical Features | Why Hobbyists Attend |\\n|------|------------------|----------------------|\\n| **Board Game CafÃ©s & Tournaments** | Free or lowâ€‘cost game access, themed nights, community building. | Play new titles, meet fellow gamers, and enjoy a relaxed environment. |\\n| **Modelâ€‘Train & Hobby Shops Events** | Buildâ€‘andâ€‘run sessions, model showcases, expert talks. | Share passion, learn new techniques, and see impressive setups. |\\n| **Language Exchange Meetâ€‘ups** | Conversation circles, cultural sharing, informal tutoring. | Practice language skills, meet native speakers, and learn about new cultures. |\\n| **DIY Electronics & Arduino Workshops** | Handsâ€‘on coding, circuit building, project showcases. | Build functional gadgets, learn coding, and network with tech hobbyists. |\\n\\n---\\n\\n## Quick â€œHowâ€‘toâ€ Checklist for Finding These Hidden Gems\\n\\n1. **Local Community Boards** â€“ Check your cityâ€™s library, community center, or university bulletin boards (physical or digital).  \\n2. **Niche Subreddits & Discord Servers** â€“ Search for subreddits like r/foodie, r/tech, r/photography, or join Discord communities that focus on your hobby.  \\n3. **Eventbrite Filters** â€“ Use the â€œInterestâ€ or â€œCategoryâ€ filters to find microâ€‘events or niche gatherings.  \\n4. **Meetup â€œInterestâ€ Tags** â€“ Look for tags like â€œmaker,â€ â€œfood,â€ â€œsports,â€ or â€œspiritual.â€  \\n5. **Local Newspapers & Magazines** â€“ Many print or online publications have a â€œCommunity Eventsâ€ section that highlights smaller, local happenings.  \\n6. **Ask Your Network** â€“ A quick â€œHey, any cool events this month?â€ can uncover hidden gems that arenâ€™t widely advertised.  \\n\\n---\\n\\n### Want to Dive Into a Specific One?\\n\\n- **Ultraâ€‘marathon in the Rockies?** I can pull the latest race dates, training tips, and gear recommendations.  \\n- **A popâ€‘up vegan cafÃ© in Brooklyn?** Iâ€™ll find the menu, opening hours, and any special events.  \\n- **A local astronomy night at the planetarium?** Iâ€™ll give you the schedule, telescope specs, and any guest astronomer details.  \\n\\nJust let me know which category or specific event type youâ€™re curious about, and Iâ€™ll dig up the freshest info for you! ðŸŒŸ'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.run(\"What kind of other events?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d1edc45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. \n",
      "The AI is talkative and provides lots of specific details from its context.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI: Hey there! ðŸ‘‹ Iâ€™m your friendly neighborhood AI, ready to chat, answer questions, or just keep you company. Iâ€™m powered by the latest GPTâ€‘4 architecture, and Iâ€™ve been trained on a massive mix of books, articles, websites, and even some fun triviaâ€”so feel free to throw anything my way. Whether youâ€™re curious about the science behind photosynthesis, need a recipe tweak, or just want to hear a joke, Iâ€™ve got you covered. \n",
      "\n",
      "By the way, itâ€™s currently November 28, 2025, so if youâ€™re planning a holiday or just wondering whatâ€™s trending this week, I can help with that too. Whatâ€™s on your mind today?\n",
      "Human: what is the most iconic place in Rome?\n",
      "AI: The single place that practically pops up on every â€œmustâ€‘seeâ€ list, in every travel brochure, and in the first thing people think of when they picture Rome is the **Colosseum** (officially the Flavian Amphitheatre). Itâ€™s the living, breathing heart of ancient Roman engineering and spectacle, and itâ€™s hard to argue with its iconic status.\n",
      "\n",
      "---\n",
      "\n",
      "## Why the Colosseum is Romeâ€™s poster child\n",
      "\n",
      "| Feature | What makes it iconic |\n",
      "|---------|----------------------|\n",
      "| **Size & Scale** | 6,000â€‘7,000 seats, 48â€‘meterâ€‘high, 16â€‘meterâ€‘wide arena. It dwarfs most modern stadiums. |\n",
      "| **Engineering Marvel** | 80,000 tons of travertine, tuff, and concrete; a complex system of vaults, arches, and a retractable awning (velarium) that could shade the entire arena. |\n",
      "| **Historical Significance** | Built 70â€‘80â€¯AD under Vespasian and Titus, it was the site of gladiatorial contests, mock naval battles, animal hunts, and public executionsâ€”events that defined Roman public life. |\n",
      "| **Cultural Resonance** | The image of the Colosseum is synonymous with Rome in films, postcards, and even the logo of the Italian national football team. |\n",
      "| **Survival Story** | Withstood earthquakes, fires, and stoneâ€‘thieves for centuries, yet still stands largely intact. |\n",
      "\n",
      "---\n",
      "\n",
      "## A quick tour of what youâ€™ll see\n",
      "\n",
      "1. **The Outer Perimeter (Piazza del Colosseo)** â€“ The massive, open square thatâ€™s a hub for locals and tourists alike. Youâ€™ll find the **Arco della Pace** (Arch of Peace) and the **Museo del Colosseo** (Colosseum Museum) right on the edge.\n",
      "\n",
      "2. **The Inner Arena** â€“ Walk down the **hypogeum** (the underground network of tunnels and chambers) to see where gladiators and animals were staged. The museumâ€™s 3â€‘D reconstruction gives you a sense of the scale and the logistics of ancient spectacles.\n",
      "\n",
      "3. **The Upper Tier** â€“ The **tesserae** (stone tiles) that once covered the seating area are still visible. Theyâ€™re a reminder of the social hierarchy: emperors, senators, and the general public all had distinct sections.\n",
      "\n",
      "4. **The Velarium** â€“ While the original retractable awning is long gone, you can see the wooden framework that supported it. Imagine the roar of the crowd and the shade of that massive canvas!\n",
      "\n",
      "---\n",
      "\n",
      "## Fun facts to sprinkle into conversation\n",
      "\n",
      "- **Name Origin**: The name â€œColosseumâ€ comes from the nearby **Colossus of Nero**, a gigantic bronze statue that once stood in front of the amphitheater. The statue was later moved to the **Ara Pacis** (Altar of Peace) in the 1st centuryâ€¯AD.\n",
      "- **Gladiator Training**: The **ludus** (gladiator school) was located just outside the Colosseum. Some of the training pits are still visible in the hypogeum.\n",
      "- **Modern Use**: In the 20th century, the Colosseum was used as a filming location for movies like *Gladiator* (2000) and *The Last Samurai* (2003). It also hosted a 2009 concert by the Italian rock band **Eros Ramazzotti**.\n",
      "- **Conservation Efforts**: The **Colosseum Restoration Project** (ongoing since the 1990s) has used laser scanning and 3â€‘D modeling to preserve the structure and plan future repairs.\n",
      "\n",
      "---\n",
      "\n",
      "## Visiting Tips\n",
      "\n",
      "| Tip | Why it matters |\n",
      "|-----|----------------|\n",
      "| **Buy tickets online** | Skip the long lines (especially in peak season). |\n",
      "| **Visit early or late** | The light is softer, and the crowds are thinner. |\n",
      "| **Take a guided tour** | A knowledgeable guide can point out hidden details (like the **tomb of the Emperor Titus** in the hypogeum). |\n",
      "| **Combine with nearby sites** | The Roman Forum and Palatine Hill are right next doorâ€”plan a halfâ€‘day itinerary. |\n",
      "| **Wear comfortable shoes** | The stone steps and uneven surfaces can be tough on feet. |\n",
      "\n",
      "---\n",
      "\n",
      "## Other iconic spots you might want to add to your list\n",
      "\n",
      "- **St.â€¯Peterâ€™s Basilica** (Vatican City) â€“ The largest church in the world, with Michelangeloâ€™s dome and Berniniâ€™s Baldachin.\n",
      "- **Trevi Fountain** â€“ Toss a coin and make a wish; the baroque masterpiece is a mustâ€‘see at night.\n",
      "- **Pantheon** â€“ A marvel of ancient Roman architecture with its massive dome and oculus.\n",
      "- **Spanish Steps** â€“ A social hub where locals and tourists mingle.\n",
      "- **Piazza Navona** â€“ A baroque square with Berniniâ€™s Fountain of the Four Rivers.\n",
      "\n",
      "---\n",
      "\n",
      "### Bottom line\n",
      "\n",
      "If youâ€™re looking for the single, unmistakable symbol of Rome that captures its ancient grandeur, its architectural genius, and its enduring cultural impact, the **Colosseum** is the place to go. Whether youâ€™re a history buff, an architecture enthusiast, or just someone who loves a good postcard, the Colosseum offers a tangible, aweâ€‘inspiring connection to the pastâ€”and a spectacular backdrop for your next Instagram photo. Enjoy your Roman adventure! ðŸ‡®ðŸ‡¹âœ¨\n",
      "Human: ai start up \n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "User:  ai start up \n",
      "AI system:  ## ðŸš€ AI Startâ€‘Up Playbook  \n",
      "*(All dates are current as ofâ€¯Novâ€¯28â€¯2025 â€“ the AI ecosystem is moving fast, so keep an eye on the latest news.)*\n",
      "\n",
      "---\n",
      "\n",
      "### 1. Why AI is a â€œGolden Ageâ€ for Startâ€‘Ups\n",
      "\n",
      "| Trend | Why it matters for founders |\n",
      "|-------|-----------------------------|\n",
      "| **Massive data availability** | 5â€¯PB+ of structured & unstructured data is now accessible via public APIs, cloud storage, and sensor networks. |\n",
      "| **Hardware democratization** | GPUs, TPUs, and edgeâ€‘AI chips are now commodity items; cloud providers offer payâ€‘asâ€‘youâ€‘go compute. |\n",
      "| **Openâ€‘source frameworks** | TensorFlow, PyTorch, Huggingâ€¯Face, and ONNX make it trivial to prototype and ship models. |\n",
      "| **Regulatory clarity** | GDPR, CCPA, and emerging AIâ€‘specific regulations (EU AI Act) provide a framework for complianceâ€‘first products. |\n",
      "| **Capital appetite** | VC funds now earmark 30â€“40â€¯% of their AI budgets for â€œhighâ€‘impactâ€ useâ€‘cases (health, fintech, climate). |\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Hot AIâ€‘Enabled Sectors (2025â€‘2027)\n",
      "\n",
      "| Sector | Typical AI Problem | Market Size (2025) | Example Startâ€‘Ups |\n",
      "|--------|--------------------|--------------------|-------------------|\n",
      "| **Healthcare** | Predictive diagnostics, drug discovery, personalized treatment | $120â€¯B (global) | PathAI, Insilico Medicine |\n",
      "| **FinTech** | Fraud detection, credit scoring, roboâ€‘advisors | $45â€¯B | Zest AI, Onfido |\n",
      "| **Climate & Energy** | Energyâ€‘efficiency optimization, carbonâ€‘tracking | $30â€¯B | CarbonCure, DeepMindâ€™s AlphaFold for materials |\n",
      "| **Retail & Eâ€‘commerce** | Recommendation engines, visual search, inventory forecasting | $200â€¯B | Dynamic Yield, Vue.ai |\n",
      "| **Legal & Compliance** | Contract analysis, regulatory monitoring | $15â€¯B | Luminance, Evisort |\n",
      "| **Education** | Adaptive learning, plagiarism detection | $10â€¯B | Duolingo, Querium |\n",
      "| **Manufacturing** | Predictive maintenance, quality inspection | $25â€¯B | SparkCognition, Seebo |\n",
      "| **Transportation** | Autonomous driving, route optimization | $70â€¯B | Waymo, Starship Technologies |\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Funding Landscape (2025)\n",
      "\n",
      "| Stage | Typical Size | Key Investors | Notes |\n",
      "|-------|--------------|---------------|-------|\n",
      "| **Seed** | $200â€¯k â€“ $1â€¯M | Angel networks, YC, Techstars, local accelerators | Focus on MVP & early traction. |\n",
      "| **Seriesâ€¯A** | $2â€¯M â€“ $10â€¯M | VC funds (Sequoia, Andreessen, Lightspeed), corporate VCs | Validate productâ€‘market fit, hire core team. |\n",
      "| **Seriesâ€¯B** | $10â€¯M â€“ $30â€¯M | Growth VCs, strategic partners | Scale sales, expand into new geographies. |\n",
      "| **Seriesâ€¯C+** | $30â€¯M+ | Lateâ€‘stage VCs, private equity | Prepare for IPO or acquisition. |\n",
      "| **Grants & Competitions** | $50â€¯k â€“ $500â€¯k | NSF, EU Horizon, Google AI Impact Challenge | Nonâ€‘equity, great for earlyâ€‘stage R&D. |\n",
      "\n",
      "---\n",
      "\n",
      "### 4. 10 AIâ€‘Startup Ideas Worth Exploring\n",
      "\n",
      "| Idea | Core AI Tech | Pain Point | MVP Validation |\n",
      "|------|--------------|------------|----------------|\n",
      "| **AIâ€‘Powered Clinical Trial Matching** | NLP + embeddings | Patients & sponsors struggle to find matches | Build a demo matching 10+ trials to 100+ patients |\n",
      "| **Realâ€‘Time Supplyâ€‘Chain Visibility** | Computer vision + edge AI | Stockouts & overstock | Deploy cameras on a warehouse and show live alerts |\n",
      "| **Personalized Learning Paths** | Reinforcement learning | Oneâ€‘sizeâ€‘fitsâ€‘all courses | Prototype a microâ€‘learning platform for a niche skill |\n",
      "| **AIâ€‘Assisted Legal Drafting** | GPTâ€‘4 + fineâ€‘tuning | Timeâ€‘consuming contract review | Offer a Chrome extension that autoâ€‘suggests clauses |\n",
      "| **Smart Energy Grid Optimizer** | Timeâ€‘series forecasting | Peakâ€‘load spikes | Simulate a 10â€‘household microgrid |\n",
      "| **Voiceâ€‘toâ€‘Code Assistant** | Speechâ€‘toâ€‘text + code generation | Lowâ€‘skill developers | Build a VSCode plugin that turns spoken specs into code |\n",
      "| **Emotionâ€‘Aware Customer Support** | Sentiment + affective computing | Low CSAT | Integrate with a liveâ€‘chat platform and measure response time |\n",
      "| **AIâ€‘Driven Urban Planning** | GIS + generative models | Inefficient zoning | Create a prototype that suggests optimal building placements |\n",
      "| **Fraudâ€‘Detection for Crypto** | Graph neural nets | Rapidly evolving attack vectors | Test on a public blockchain dataset |\n",
      "| **AIâ€‘Enhanced Mentalâ€‘Health Chatbot** | Conversational AI + safety monitoring | Accessibility gaps | Pilot with a small group of users and track engagement |\n",
      "\n",
      "---\n",
      "\n",
      "### 5. Stepâ€‘byâ€‘Step Roadmap\n",
      "\n",
      "| Phase | Key Milestones | Deliverables |\n",
      "|-------|----------------|--------------|\n",
      "| **Idea & Validation** | Market research, customer interviews, problemâ€‘fit | Problem statement, validated user personas |\n",
      "| **MVP Development** | Build core AI model, minimal UI, data pipeline | Working prototype, demo video |\n",
      "| **Pilot & Feedback** | Deploy to 5â€“10 early adopters, collect metrics | User feedback report, iteration plan |\n",
      "| **Seed Funding** | Pitch deck, financial model, term sheet | Seed round closed, team hires |\n",
      "| **Growth & Scale** | Expand product features, sales pipeline, hiring | Seriesâ€¯A deck, revenue projections |\n",
      "| **Scale & Exit** | International expansion, IPO prep or acquisition | Board of directors, legal counsel |\n",
      "\n",
      "---\n",
      "\n",
      "### 6. Essential Resources\n",
      "\n",
      "| Category | Top Picks |\n",
      "|----------|-----------|\n",
      "| **Accelerators** | Y Combinator, Techstars, 500 Startups, Plug and Play AI |\n",
      "| **Incubators** | MIT Media Lab, Stanford AI Lab, NVIDIA Inception |\n",
      "| **Funding Platforms** | AngelList, SeedInvest, Crunchbase |\n",
      "| **Data Sources** | Kaggle, UCI ML Repository, Open Data Network |\n",
      "| **Model Repositories** | Huggingâ€¯Face Model Hub, TensorFlow Hub, PyTorch Hub |\n",
      "| **Compliance Guides** | EU AI Act whitepapers, GDPR AI Toolkit |\n",
      "| **Community** | r/MachineLearning, AI Alignment Forum, Deep Learning AI Discord |\n",
      "\n",
      "---\n",
      "\n",
      "### 7. Common Pitfalls & How to Avoid Them\n",
      "\n",
      "| Pitfall | Why it Happens | Mitigation |\n",
      "|---------|----------------|------------|\n",
      "| **Overâ€‘engineering the AI** | Belief that â€œmore AI = betterâ€ | Start with a simple ruleâ€‘based baseline; iterate |\n",
      "| **Ignoring data quality** | Garbage in, garbage out | Build a data governance framework early |\n",
      "| **Skipping user research** | Assuming AI solves the problem | Conduct at least 20 user interviews before coding |\n",
      "| **Underestimating regulatory hurdles** | AI in regulated sectors | Hire a compliance officer or partner with a legal firm |\n",
      "| **Failing to plan for bias** | Models trained on skewed data | Use biasâ€‘audit tools (AI Fairness 360, Fairlearn) |\n",
      "| **Neglecting explainability** | Clients demand transparency | Integrate SHAP or LIME into the product |\n",
      "| **Burning through runway** | Overâ€‘spending on talent | Adopt a lean hiring strategy; use contractors |\n",
      "\n",
      "---\n",
      "\n",
      "### 8. Success Factors (What VCs Look For)\n",
      "\n",
      "| Factor | How to Demonstrate |\n",
      "|--------|--------------------|\n",
      "| **Clear Market Size** | TAM/SAM/SOM metrics, growth rates |\n",
      "| **Scalable AI** | Model that can be retrained on new data with minimal cost |\n",
      "| **Strong IP** | Patents, proprietary datasets, unique algorithms |\n",
      "| **Traction** | Monthly recurring revenue (MRR), churn <â€¯5â€¯% |\n",
      "| **Team** | Complementary skills (AI, product, sales) |\n",
      "| **Exit Potential** | Clear path to acquisition or IPO |\n",
      "\n",
      "---\n",
      "\n",
      "### 9. Key Metrics to Track\n",
      "\n",
      "| Metric | Why It Matters | Target (Early Stage) |\n",
      "|--------|----------------|----------------------|\n",
      "| **Data Velocity** | Speed of data ingestion | <â€¯5â€¯min for realâ€‘time use |\n",
      "| **Model Accuracy** | Business impact | â‰¥â€¯90â€¯% for classification tasks |\n",
      "| **Cost per Prediction** | Operational efficiency | <$0.01 for cloud inference |\n",
      "| **Customer Acquisition Cost (CAC)** | Growth sustainability | â‰¤â€¯$200 for B2B SaaS |\n",
      "| **Lifetime Value (LTV)** | Revenue potential | â‰¥â€¯3Ã— CAC |\n",
      "| **Churn Rate** | Retention | <â€¯5â€¯% per month |\n",
      "| **Model Drift** | Reliability | <â€¯2â€¯% change in performance over 30â€¯days |\n",
      "\n",
      "---\n",
      "\n",
      "### 10. Funding & Grants to Watch\n",
      "\n",
      "| Program | Eligibility | Amount | Deadline |\n",
      "|---------|-------------|--------|----------|\n",
      "| **NSF AI Research** | U.S. researchers | $500â€¯kâ€“$2â€¯M | 12â€¯Octâ€¯2025 |\n",
      "| **EU Horizon Europe â€“ AI** | EU & partner countries | â‚¬1â€“â‚¬5â€¯M | 30â€¯Sepâ€¯2025 |\n",
      "| **Google AI Impact Challenge** | Global | $50â€¯kâ€“$500â€¯k | 15â€¯Novâ€¯2025 |\n",
      "| **Microsoft AI for Good** | Social impact | $100â€¯kâ€“$1â€¯M | 31â€¯Decâ€¯2025 |\n",
      "| **DARPA AI Next** | U.S. defense | $1â€“$10â€¯M | 30â€¯Janâ€¯2026 |\n",
      "\n",
      "---\n",
      "\n",
      "### 11. Top AIâ€‘Focused Conferences (2025)\n",
      "\n",
      "| Conference | Focus | Location | Dates |\n",
      "|------------|-------|----------|-------|\n",
      "| **NeurIPS** | Deep learning research | Vancouver | Decâ€¯2025 |\n",
      "| **ICML** | Machine learning theory | Beijing | Julâ€¯2025 |\n",
      "| **CVPR** | Computer vision | New Orleans | Junâ€¯2025 |\n",
      "| **AAAI** | AI & society | New York | Febâ€¯2025 |\n",
      "| **AI Expo Global** | Enterprise AI | London | Octâ€¯2025 |\n",
      "| **AI Summit** | AI in business | San Francisco | Marâ€¯2025 |\n",
      "\n",
      "---\n",
      "\n",
      "### 12. Podcasts & Blogs to Follow\n",
      "\n",
      "| Podcast | Focus | Link |\n",
      "|---------|-------|------|\n",
      "| **Lex Fridman Podcast** | AI & philosophy | https://lexfridman.com/podcast |\n",
      "| **The AI Alignment Podcast** | Safety & ethics | https://ai-alignment.org/podcast |\n",
      "| **Data Skeptic** | Data science & AI | https://dataskeptic.com |\n",
      "| **Machine Learning Street Talk** | Deep learning trends | https://mlst.ai |\n",
      "| **Towards Data Science** | Practical AI | https://towardsdatascience.com |\n",
      "\n",
      "---\n",
      "\n",
      "### 13. Mustâ€‘Read Books (2024â€‘2025)\n",
      "\n",
      "| Title | Author | Why Itâ€™s Useful |\n",
      "|-------|--------|-----------------|\n",
      "| *Artificial Intelligence: A Modern Approach* | Russell & Norvig | Foundational concepts |\n",
      "| *Deep Learning* | Goodfellow, Bengio, Courville | Technical depth |\n",
      "| *The Alignment Problem* | Brian Christian | Ethics & safety |\n",
      "| *AI Superpowers* | Kai-Fu Lee | Market dynamics |\n",
      "| *Prediction Machines* | Ajay Agrawal | Business strategy |\n",
      "\n",
      "---\n",
      "\n",
      "### 14. Online Courses & Bootcamps\n",
      "\n",
      "| Course | Platform | Focus | Duration |\n",
      "|--------|----------|-------|----------|\n",
      "| **CS231n** | Stanford | CNNs & vision | 12â€¯weeks |\n",
      "| **Fast.ai** | fast.ai | Practical deep learning | 8â€¯weeks |\n",
      "| **DeepLearning.AI TensorFlow Developer** | Coursera | TensorFlow | 4â€¯weeks |\n",
      "| **AI for Everyone** | Coursera | Business & ethics | 4â€¯weeks |\n",
      "| **DataCamp Data Scientist with Python** | DataCamp | Data pipeline | 6â€¯weeks |\n",
      "\n",
      "---\n",
      "\n",
      "### 15. Mentors & Advisory Boards\n",
      "\n",
      "| Mentor | Background | How to Connect |\n",
      "|--------|------------|----------------|\n",
      "| **Andrew Ng** | DeepLearning.AI, Coursera | LinkedIn, AI conferences |\n",
      "| **Fei-Fei Li** | Stanford, AI4ALL | Academic collaborations |\n",
      "| **Yann LeCun** | Facebook AI | AI conferences, research labs |\n",
      "| **Cynthia Breazeal** | MIT, social robotics | MIT AI Lab events |\n",
      "| **Ginni Rometty** | Former IBM CEO | AI industry panels |\n",
      "\n",
      "---\n",
      "\n",
      "### 16. Quickâ€‘Start Checklist (First 90â€¯Days)\n",
      "\n",
      "1. **Validate the Problem** â€“ 10+ interviews, problemâ€‘fit score â‰¥â€¯8/10.  \n",
      "2. **Prototype the AI** â€“ 1â€‘month proofâ€‘ofâ€‘concept with openâ€‘source models.  \n",
      "3. **Build a Landing Page** â€“ Capture emails, show demo video.  \n",
      "4. **Set Up a Data Pipeline** â€“ Use cloud storage + ETL tools (Airflow, Prefect).  \n",
      "5. **Create a Minimal Viable Product** â€“ 2â€‘week sprint, deploy on Heroku or AWS.  \n",
      "6. **Run a Pilot** â€“ 5â€“10 beta users, collect usage metrics.  \n",
      "7. **Iterate** â€“ Fix bugs, improve model accuracy.  \n",
      "8. **Pitch to Angel Investors** â€“ Prepare a 10â€‘slide deck.  \n",
      "9. **Apply to an Accelerator** â€“ Submit to Y Combinator or Techstars.  \n",
      "10. **Launch Public Beta** â€“ 30â€¯days of realâ€‘world usage, track churn.\n",
      "\n",
      "---\n",
      "\n",
      "### 17. Final Thought\n",
      "\n",
      "Building an AI startâ€‘up is a blend of **technical excellence** and **business acumen**. The most successful founders are those who:\n",
      "\n",
      "- **Listen** to real users before coding.  \n",
      "- **Iterate** quickly, treating the AI model as a product that can be tuned.  \n",
      "- **Plan** for compliance and ethics from day one.  \n",
      "- **Network** relentlesslyâ€”every conference, every meetup, every paper can be a seed for partnership or funding.\n",
      "\n",
      "Good luck, and may your models be accurate, your data clean, and your runway long! ðŸš€\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "# Make sure you have imported the necessary memory and LLM classes (e.g., ConversationBufferMemory, ChatOpenAI)\n",
    "\n",
    "# 1. Define a custom prompt template that uses {question} instead of {input}\n",
    "custom_template = \"\"\"The following is a friendly conversation between a human and an AI. \n",
    "The AI is talkative and provides lots of specific details from its context.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Human: {question}\n",
    "AI:\"\"\"\n",
    "\n",
    "custom_prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"question\"], \n",
    "    template=custom_template\n",
    ")\n",
    "\n",
    "# 2. Initialize the ConversationChain, passing your custom prompt AND setting input_key='question'\n",
    "# (Assuming 'chat' and 'memory' variables are defined elsewhere in your code)\n",
    "conversation = ConversationChain(\n",
    "    llm=chat, \n",
    "    verbose=True, \n",
    "    memory=memory,\n",
    "    prompt=custom_prompt,      # <-- Use the custom prompt\n",
    "    input_key='question'       # <-- Match the input key to the prompt variable name\n",
    ")\n",
    "\n",
    "# Your while loop will now work correctly:\n",
    "while True:\n",
    "    query = input('you: ')\n",
    "    if query == 'q':\n",
    "        break\n",
    "    \n",
    "    # Assuming you implemented the fix with the 'question' input key from before\n",
    "    output = conversation({\"question\": query}) \n",
    "    print('User: ', query)\n",
    "    \n",
    "    # FIX: Try accessing the correct output key, which is often 'output_text' or 'response'\n",
    "    # Use .get() to safely check which key exists if unsure\n",
    "    ai_response = output.get('output_text') or output.get('response') or output.get('text') \n",
    "    \n",
    "    if ai_response:\n",
    "        print('AI system: ', ai_response)\n",
    "    else:\n",
    "        # If all else fails, print the whole dict to debug the keys\n",
    "        print('Error: Could not find AI response key.')\n",
    "        print('DEBUG OUTPUT DICTIONARY:', output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150fcd2f",
   "metadata": {},
   "source": [
    "### Adding non parametric knowledge\n",
    "- enabling to connect the model with the world data on the retrieval or  vectorDB\n",
    "- we can use agent in this case to choose the best non parametric knowledge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5849181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1500,\n",
    "            chunk_overlap=200\n",
    "        )\n",
    "\n",
    "raw_documents = PyPDFLoader('Data/italy_travel.pdf').load()\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "db = FAISS.from_documents(documents, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaf4029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(memory_key = 'chat_history',return_messages=True)\n",
    "llm = OpenAI()\n",
    "qa_chat = ConversationalRetrievalChain(llm, retriever = db.as_retriever(),memory=memory,verbose = True)\n",
    "qa_chat.run({'question':'give me some review about the pantheon'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
